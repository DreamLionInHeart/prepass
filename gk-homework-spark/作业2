> select distinct id staffid, name staffname, 'abc' level from (select id,name from (select distinct id, name from (select id,name,max(salary) from employee group by id,name) where id = '1001') except select id2, name2 from employee2) where id = '1001';
21/09/05 23:02:38 WARN PlanChangeLogger: Batch Substitution has no effect.
21/09/05 23:02:38 WARN PlanChangeLogger: Batch Disable Hints has no effect.
21/09/05 23:02:38 WARN PlanChangeLogger: Batch Hints has no effect.
21/09/05 23:02:38 WARN PlanChangeLogger: Batch Simple Sanity Check has no effect.
21/09/05 23:02:38 WARN PlanChangeLogger: 
=== Applying Rule org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations ===
 'Distinct                                                                                                      'Distinct
 +- 'Project ['id AS staffid#542, 'name AS staffname#543, abc AS level#544]                                     +- 'Project ['id AS staffid#542, 'name AS staffname#543, abc AS level#544]
    +- 'Filter ('id = 1001)                                                                                        +- 'Filter ('id = 1001)
       +- 'SubqueryAlias __auto_generated_subquery_name                                                               +- 'SubqueryAlias __auto_generated_subquery_name
          +- 'Except false                                                                                               +- 'Except false
             :- 'Project ['id, 'name]                                                                                       :- 'Project ['id, 'name]
             :  +- 'SubqueryAlias __auto_generated_subquery_name                                                            :  +- 'SubqueryAlias __auto_generated_subquery_name
             :     +- 'Distinct                                                                                             :     +- 'Distinct
             :        +- 'Project ['id, 'name]                                                                              :        +- 'Project ['id, 'name]
             :           +- 'Filter ('id = 1001)                                                                            :           +- 'Filter ('id = 1001)
             :              +- 'SubqueryAlias __auto_generated_subquery_name                                                :              +- 'SubqueryAlias __auto_generated_subquery_name
             :                 +- 'Aggregate ['id, 'name], ['id, 'name, unresolvedalias('max('salary), None)]               :                 +- 'Aggregate ['id, 'name], ['id, 'name, unresolvedalias('max('salary), None)]
!            :                    +- 'UnresolvedRelation [employee], [], false                                              :                    +- 'SubqueryAlias spark_catalog.default.employee
!            +- 'Project ['id2, 'name2]                                                                                     :                       +- 'UnresolvedCatalogRelation `default`.`employee`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [], false
!               +- 'UnresolvedRelation [employee2], [], false                                                               +- 'Project ['id2, 'name2]
!                                                                                                                              +- 'SubqueryAlias spark_catalog.default.employee2
!                                                                                                                                 +- 'UnresolvedCatalogRelation `default`.`employee2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [], false
           
21/09/05 23:02:38 WARN PlanChangeLogger: 
=== Applying Rule org.apache.spark.sql.execution.datasources.FindDataSourceTable ===
 'Distinct                                                                                                                                               'Distinct
 +- 'Project ['id AS staffid#542, 'name AS staffname#543, abc AS level#544]                                                                              +- 'Project ['id AS staffid#542, 'name AS staffname#543, abc AS level#544]
    +- 'Filter ('id = 1001)                                                                                                                                 +- 'Filter ('id = 1001)
       +- 'SubqueryAlias __auto_generated_subquery_name                                                                                                        +- 'SubqueryAlias __auto_generated_subquery_name
          +- 'Except false                                                                                                                                        +- 'Except false
             :- 'Project ['id, 'name]                                                                                                                                :- 'Project ['id, 'name]
             :  +- 'SubqueryAlias __auto_generated_subquery_name                                                                                                     :  +- 'SubqueryAlias __auto_generated_subquery_name
             :     +- 'Distinct                                                                                                                                      :     +- 'Distinct
             :        +- 'Project ['id, 'name]                                                                                                                       :        +- 'Project ['id, 'name]
             :           +- 'Filter ('id = 1001)                                                                                                                     :           +- 'Filter ('id = 1001)
             :              +- 'SubqueryAlias __auto_generated_subquery_name                                                                                         :              +- 'SubqueryAlias __auto_generated_subquery_name
             :                 +- 'Aggregate ['id, 'name], ['id, 'name, unresolvedalias('max('salary), None)]                                                        :                 +- 'Aggregate ['id, 'name], ['id, 'name, unresolvedalias('max('salary), None)]
!            :                    +- 'SubqueryAlias spark_catalog.default.employee                                                                                   :                    +- SubqueryAlias spark_catalog.default.employee
!            :                       +- 'UnresolvedCatalogRelation `default`.`employee`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [], false               :                       +- HiveTableRelation [`default`.`employee`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#545, name#546, salary#547, departmentid#548], Partition Cols: []]
             +- 'Project ['id2, 'name2]                                                                                                                              +- 'Project ['id2, 'name2]
!               +- 'SubqueryAlias spark_catalog.default.employee2                                                                                                       +- SubqueryAlias spark_catalog.default.employee2
!                  +- 'UnresolvedCatalogRelation `default`.`employee2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, [], false                                      +- HiveTableRelation [`default`.`employee2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id2#549, name2#550, salary#551], Partition Cols: []]
           
21/09/05 23:02:38 WARN PlanChangeLogger: 
=== Applying Rule org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences ===
 'Distinct                                                                                                                                                                                                              'Distinct
 +- 'Project ['id AS staffid#542, 'name AS staffname#543, abc AS level#544]                                                                                                                                             +- 'Project ['id AS staffid#542, 'name AS staffname#543, abc AS level#544]
    +- 'Filter ('id = 1001)                                                                                                                                                                                                +- 'Filter ('id = 1001)
       +- 'SubqueryAlias __auto_generated_subquery_name                                                                                                                                                                       +- 'SubqueryAlias __auto_generated_subquery_name
          +- 'Except false                                                                                                                                                                                                       +- 'Except false
             :- 'Project ['id, 'name]                                                                                                                                                                                               :- 'Project ['id, 'name]
             :  +- 'SubqueryAlias __auto_generated_subquery_name                                                                                                                                                                    :  +- 'SubqueryAlias __auto_generated_subquery_name
             :     +- 'Distinct                                                                                                                                                                                                     :     +- 'Distinct
             :        +- 'Project ['id, 'name]                                                                                                                                                                                      :        +- 'Project ['id, 'name]
             :           +- 'Filter ('id = 1001)                                                                                                                                                                                    :           +- 'Filter ('id = 1001)
             :              +- 'SubqueryAlias __auto_generated_subquery_name                                                                                                                                                        :              +- 'SubqueryAlias __auto_generated_subquery_name
!            :                 +- 'Aggregate ['id, 'name], ['id, 'name, unresolvedalias('max('salary), None)]                                                                                                                       :                 +- 'Aggregate [id#545, name#546], [id#545, name#546, unresolvedalias('max(salary#547), None)]
             :                    +- SubqueryAlias spark_catalog.default.employee                                                                                                                                                   :                    +- SubqueryAlias spark_catalog.default.employee
             :                       +- HiveTableRelation [`default`.`employee`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#545, name#546, salary#547, departmentid#548], Partition Cols: []]               :                       +- HiveTableRelation [`default`.`employee`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#545, name#546, salary#547, departmentid#548], Partition Cols: []]
!            +- 'Project ['id2, 'name2]                                                                                                                                                                                             +- Project [id2#549, name2#550]
                +- SubqueryAlias spark_catalog.default.employee2                                                                                                                                                                       +- SubqueryAlias spark_catalog.default.employee2
                   +- HiveTableRelation [`default`.`employee2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id2#549, name2#550, salary#551], Partition Cols: []]                                                      +- HiveTableRelation [`default`.`employee2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id2#549, name2#550, salary#551], Partition Cols: []]
           
21/09/05 23:02:38 WARN PlanChangeLogger: 
=== Applying Rule org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions ===
 'Distinct                                                                                                                                                                                                              'Distinct
 +- 'Project ['id AS staffid#542, 'name AS staffname#543, abc AS level#544]                                                                                                                                             +- 'Project ['id AS staffid#542, 'name AS staffname#543, abc AS level#544]
    +- 'Filter ('id = 1001)                                                                                                                                                                                                +- 'Filter ('id = 1001)
       +- 'SubqueryAlias __auto_generated_subquery_name                                                                                                                                                                       +- 'SubqueryAlias __auto_generated_subquery_name
          +- 'Except false                                                                                                                                                                                                       +- 'Except false
             :- 'Project ['id, 'name]                                                                                                                                                                                               :- 'Project ['id, 'name]
             :  +- 'SubqueryAlias __auto_generated_subquery_name                                                                                                                                                                    :  +- 'SubqueryAlias __auto_generated_subquery_name
             :     +- 'Distinct                                                                                                                                                                                                     :     +- 'Distinct
             :        +- 'Project ['id, 'name]                                                                                                                                                                                      :        +- 'Project ['id, 'name]
             :           +- 'Filter ('id = 1001)                                                                                                                                                                                    :           +- 'Filter ('id = 1001)
             :              +- 'SubqueryAlias __auto_generated_subquery_name                                                                                                                                                        :              +- 'SubqueryAlias __auto_generated_subquery_name
!            :                 +- 'Aggregate [id#545, name#546], [id#545, name#546, unresolvedalias('max(salary#547), None)]                                                                                                        :                 +- 'Aggregate [id#545, name#546], [id#545, name#546, unresolvedalias(max(salary#547), None)]
             :                    +- SubqueryAlias spark_catalog.default.employee                                                                                                                                                   :                    +- SubqueryAlias spark_catalog.default.employee
             :                       +- HiveTableRelation [`default`.`employee`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#545, name#546, salary#547, departmentid#548], Partition Cols: []]               :                       +- HiveTableRelation [`default`.`employee`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#545, name#546, salary#547, departmentid#548], Partition Cols: []]
             +- Project [id2#549, name2#550]                                                                                                                                                                                        +- Project [id2#549, name2#550]
                +- SubqueryAlias spark_catalog.default.employee2                                                                                                                                                                       +- SubqueryAlias spark_catalog.default.employee2
                   +- HiveTableRelation [`default`.`employee2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id2#549, name2#550, salary#551], Partition Cols: []]                                                      +- HiveTableRelation [`default`.`employee2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id2#549, name2#550, salary#551], Partition Cols: []]
           
21/09/05 23:02:38 WARN PlanChangeLogger: 
=== Applying Rule org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveAliases ===
 'Distinct                                                                                                                                                                                                              'Distinct
 +- 'Project ['id AS staffid#542, 'name AS staffname#543, abc AS level#544]                                                                                                                                             +- 'Project ['id AS staffid#542, 'name AS staffname#543, abc AS level#544]
    +- 'Filter ('id = 1001)                                                                                                                                                                                                +- 'Filter ('id = 1001)
       +- 'SubqueryAlias __auto_generated_subquery_name                                                                                                                                                                       +- 'SubqueryAlias __auto_generated_subquery_name
          +- 'Except false                                                                                                                                                                                                       +- 'Except false
             :- 'Project ['id, 'name]                                                                                                                                                                                               :- 'Project ['id, 'name]
             :  +- 'SubqueryAlias __auto_generated_subquery_name                                                                                                                                                                    :  +- 'SubqueryAlias __auto_generated_subquery_name
             :     +- 'Distinct                                                                                                                                                                                                     :     +- 'Distinct
             :        +- 'Project ['id, 'name]                                                                                                                                                                                      :        +- 'Project ['id, 'name]
             :           +- 'Filter ('id = 1001)                                                                                                                                                                                    :           +- 'Filter ('id = 1001)
!            :              +- 'SubqueryAlias __auto_generated_subquery_name                                                                                                                                                        :              +- SubqueryAlias __auto_generated_subquery_name
!            :                 +- 'Aggregate [id#545, name#546], [id#545, name#546, unresolvedalias(max(salary#547), None)]                                                                                                         :                 +- Aggregate [id#545, name#546], [id#545, name#546, max(salary#547) AS max(salary)#553]
             :                    +- SubqueryAlias spark_catalog.default.employee                                                                                                                                                   :                    +- SubqueryAlias spark_catalog.default.employee
             :                       +- HiveTableRelation [`default`.`employee`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#545, name#546, salary#547, departmentid#548], Partition Cols: []]               :                       +- HiveTableRelation [`default`.`employee`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#545, name#546, salary#547, departmentid#548], Partition Cols: []]
             +- Project [id2#549, name2#550]                                                                                                                                                                                        +- Project [id2#549, name2#550]
                +- SubqueryAlias spark_catalog.default.employee2                                                                                                                                                                       +- SubqueryAlias spark_catalog.default.employee2
                   +- HiveTableRelation [`default`.`employee2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id2#549, name2#550, salary#551], Partition Cols: []]                                                      +- HiveTableRelation [`default`.`employee2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id2#549, name2#550, salary#551], Partition Cols: []]
           
21/09/05 23:02:38 WARN PlanChangeLogger: 
=== Applying Rule org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences ===
 'Distinct                                                                                                                                                                                                              'Distinct
 +- 'Project ['id AS staffid#542, 'name AS staffname#543, abc AS level#544]                                                                                                                                             +- 'Project ['id AS staffid#542, 'name AS staffname#543, abc AS level#544]
    +- 'Filter ('id = 1001)                                                                                                                                                                                                +- 'Filter ('id = 1001)
       +- 'SubqueryAlias __auto_generated_subquery_name                                                                                                                                                                       +- 'SubqueryAlias __auto_generated_subquery_name
          +- 'Except false                                                                                                                                                                                                       +- 'Except false
             :- 'Project ['id, 'name]                                                                                                                                                                                               :- 'Project ['id, 'name]
             :  +- 'SubqueryAlias __auto_generated_subquery_name                                                                                                                                                                    :  +- 'SubqueryAlias __auto_generated_subquery_name
             :     +- 'Distinct                                                                                                                                                                                                     :     +- 'Distinct
             :        +- 'Project ['id, 'name]                                                                                                                                                                                      :        +- 'Project ['id, 'name]
!            :           +- 'Filter ('id = 1001)                                                                                                                                                                                    :           +- 'Filter (id#545 = 1001)
             :              +- SubqueryAlias __auto_generated_subquery_name                                                                                                                                                         :              +- SubqueryAlias __auto_generated_subquery_name
             :                 +- Aggregate [id#545, name#546], [id#545, name#546, max(salary#547) AS max(salary)#553]                                                                                                              :                 +- Aggregate [id#545, name#546], [id#545, name#546, max(salary#547) AS max(salary)#553]
             :                    +- SubqueryAlias spark_catalog.default.employee                                                                                                                                                   :                    +- SubqueryAlias spark_catalog.default.employee
             :                       +- HiveTableRelation [`default`.`employee`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#545, name#546, salary#547, departmentid#548], Partition Cols: []]               :                       +- HiveTableRelation [`default`.`employee`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#545, name#546, salary#547, departmentid#548], Partition Cols: []]
             +- Project [id2#549, name2#550]                                                                                                                                                                                        +- Project [id2#549, name2#550]
                +- SubqueryAlias spark_catalog.default.employee2                                                                                                                                                                       +- SubqueryAlias spark_catalog.default.employee2
                   +- HiveTableRelation [`default`.`employee2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id2#549, name2#550, salary#551], Partition Cols: []]                                                      +- HiveTableRelation [`default`.`employee2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id2#549, name2#550, salary#551], Partition Cols: []]
           
21/09/05 23:02:38 WARN PlanChangeLogger: 
=== Applying Rule org.apache.spark.sql.catalyst.analysis.TypeCoercionBase$CombinedTypeCoercionRule ===
 'Distinct                                                                                                                                                                                                              'Distinct
 +- 'Project ['id AS staffid#542, 'name AS staffname#543, abc AS level#544]                                                                                                                                             +- 'Project ['id AS staffid#542, 'name AS staffname#543, abc AS level#544]
    +- 'Filter ('id = 1001)                                                                                                                                                                                                +- 'Filter ('id = 1001)
       +- 'SubqueryAlias __auto_generated_subquery_name                                                                                                                                                                       +- 'SubqueryAlias __auto_generated_subquery_name
          +- 'Except false                                                                                                                                                                                                       +- 'Except false
             :- 'Project ['id, 'name]                                                                                                                                                                                               :- 'Project ['id, 'name]
             :  +- 'SubqueryAlias __auto_generated_subquery_name                                                                                                                                                                    :  +- 'SubqueryAlias __auto_generated_subquery_name
             :     +- 'Distinct                                                                                                                                                                                                     :     +- 'Distinct
             :        +- 'Project ['id, 'name]                                                                                                                                                                                      :        +- 'Project ['id, 'name]
!            :           +- 'Filter (id#545 = 1001)                                                                                                                                                                                 :           +- Filter (id#545 = cast(1001 as int))
             :              +- SubqueryAlias __auto_generated_subquery_name                                                                                                                                                         :              +- SubqueryAlias __auto_generated_subquery_name
             :                 +- Aggregate [id#545, name#546], [id#545, name#546, max(salary#547) AS max(salary)#553]                                                                                                              :                 +- Aggregate [id#545, name#546], [id#545, name#546, max(salary#547) AS max(salary)#553]
             :                    +- SubqueryAlias spark_catalog.default.employee                                                                                                                                                   :                    +- SubqueryAlias spark_catalog.default.employee
             :                       +- HiveTableRelation [`default`.`employee`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#545, name#546, salary#547, departmentid#548], Partition Cols: []]               :                       +- HiveTableRelation [`default`.`employee`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#545, name#546, salary#547, departmentid#548], Partition Cols: []]
             +- Project [id2#549, name2#550]                                                                                                                                                                                        +- Project [id2#549, name2#550]
                +- SubqueryAlias spark_catalog.default.employee2                                                                                                                                                                       +- SubqueryAlias spark_catalog.default.employee2
                   +- HiveTableRelation [`default`.`employee2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id2#549, name2#550, salary#551], Partition Cols: []]                                                      +- HiveTableRelation [`default`.`employee2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id2#549, name2#550, salary#551], Partition Cols: []]
           
21/09/05 23:02:38 WARN PlanChangeLogger: 
=== Applying Rule org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences ===
 'Distinct                                                                                                                                                                                                              'Distinct
 +- 'Project ['id AS staffid#542, 'name AS staffname#543, abc AS level#544]                                                                                                                                             +- 'Project ['id AS staffid#542, 'name AS staffname#543, abc AS level#544]
    +- 'Filter ('id = 1001)                                                                                                                                                                                                +- 'Filter ('id = 1001)
       +- 'SubqueryAlias __auto_generated_subquery_name                                                                                                                                                                       +- 'SubqueryAlias __auto_generated_subquery_name
          +- 'Except false                                                                                                                                                                                                       +- 'Except false
!            :- 'Project ['id, 'name]                                                                                                                                                                                               :- Project [id#545, name#546]
!            :  +- 'SubqueryAlias __auto_generated_subquery_name                                                                                                                                                                    :  +- SubqueryAlias __auto_generated_subquery_name
!            :     +- 'Distinct                                                                                                                                                                                                     :     +- Distinct
!            :        +- 'Project ['id, 'name]                                                                                                                                                                                      :        +- Project [id#545, name#546]
             :           +- Filter (id#545 = cast(1001 as int))                                                                                                                                                                     :           +- Filter (id#545 = cast(1001 as int))
             :              +- SubqueryAlias __auto_generated_subquery_name                                                                                                                                                         :              +- SubqueryAlias __auto_generated_subquery_name
             :                 +- Aggregate [id#545, name#546], [id#545, name#546, max(salary#547) AS max(salary)#553]                                                                                                              :                 +- Aggregate [id#545, name#546], [id#545, name#546, max(salary#547) AS max(salary)#553]
             :                    +- SubqueryAlias spark_catalog.default.employee                                                                                                                                                   :                    +- SubqueryAlias spark_catalog.default.employee
             :                       +- HiveTableRelation [`default`.`employee`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#545, name#546, salary#547, departmentid#548], Partition Cols: []]               :                       +- HiveTableRelation [`default`.`employee`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#545, name#546, salary#547, departmentid#548], Partition Cols: []]
             +- Project [id2#549, name2#550]                                                                                                                                                                                        +- Project [id2#549, name2#550]
                +- SubqueryAlias spark_catalog.default.employee2                                                                                                                                                                       +- SubqueryAlias spark_catalog.default.employee2
                   +- HiveTableRelation [`default`.`employee2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id2#549, name2#550, salary#551], Partition Cols: []]                                                      +- HiveTableRelation [`default`.`employee2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id2#549, name2#550, salary#551], Partition Cols: []]
           
21/09/05 23:02:38 WARN PlanChangeLogger: 
=== Applying Rule org.apache.spark.sql.catalyst.analysis.ResolveTimeZone ===
 'Distinct                                                                                                                                                                                                              'Distinct
 +- 'Project ['id AS staffid#542, 'name AS staffname#543, abc AS level#544]                                                                                                                                             +- 'Project ['id AS staffid#542, 'name AS staffname#543, abc AS level#544]
    +- 'Filter ('id = 1001)                                                                                                                                                                                                +- 'Filter ('id = 1001)
       +- 'SubqueryAlias __auto_generated_subquery_name                                                                                                                                                                       +- 'SubqueryAlias __auto_generated_subquery_name
          +- 'Except false                                                                                                                                                                                                       +- 'Except false
             :- Project [id#545, name#546]                                                                                                                                                                                          :- Project [id#545, name#546]
             :  +- SubqueryAlias __auto_generated_subquery_name                                                                                                                                                                     :  +- SubqueryAlias __auto_generated_subquery_name
             :     +- Distinct                                                                                                                                                                                                      :     +- Distinct
             :        +- Project [id#545, name#546]                                                                                                                                                                                 :        +- Project [id#545, name#546]
             :           +- Filter (id#545 = cast(1001 as int))                                                                                                                                                                     :           +- Filter (id#545 = cast(1001 as int))
             :              +- SubqueryAlias __auto_generated_subquery_name                                                                                                                                                         :              +- SubqueryAlias __auto_generated_subquery_name
             :                 +- Aggregate [id#545, name#546], [id#545, name#546, max(salary#547) AS max(salary)#553]                                                                                                              :                 +- Aggregate [id#545, name#546], [id#545, name#546, max(salary#547) AS max(salary)#553]
             :                    +- SubqueryAlias spark_catalog.default.employee                                                                                                                                                   :                    +- SubqueryAlias spark_catalog.default.employee
             :                       +- HiveTableRelation [`default`.`employee`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#545, name#546, salary#547, departmentid#548], Partition Cols: []]               :                       +- HiveTableRelation [`default`.`employee`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#545, name#546, salary#547, departmentid#548], Partition Cols: []]
             +- Project [id2#549, name2#550]                                                                                                                                                                                        +- Project [id2#549, name2#550]
                +- SubqueryAlias spark_catalog.default.employee2                                                                                                                                                                       +- SubqueryAlias spark_catalog.default.employee2
                   +- HiveTableRelation [`default`.`employee2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id2#549, name2#550, salary#551], Partition Cols: []]                                                      +- HiveTableRelation [`default`.`employee2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id2#549, name2#550, salary#551], Partition Cols: []]
           
21/09/05 23:02:38 WARN PlanChangeLogger: 
=== Applying Rule org.apache.spark.sql.catalyst.analysis.TypeCoercionBase$WidenSetOperationTypes ===
 'Distinct                                                                                                                                                                                                              'Distinct
 +- 'Project ['id AS staffid#542, 'name AS staffname#543, abc AS level#544]                                                                                                                                             +- 'Project ['id AS staffid#542, 'name AS staffname#543, abc AS level#544]
    +- 'Filter ('id = 1001)                                                                                                                                                                                                +- 'Filter ('id = 1001)
!      +- 'SubqueryAlias __auto_generated_subquery_name                                                                                                                                                                       +- SubqueryAlias __auto_generated_subquery_name
!         +- 'Except false                                                                                                                                                                                                       +- Except false
!            :- Project [id#545, name#546]                                                                                                                                                                                          :- Project [cast(id#545 as string) AS id#554, name#546]
!            :  +- SubqueryAlias __auto_generated_subquery_name                                                                                                                                                                     :  +- Project [id#545, name#546]
!            :     +- Distinct                                                                                                                                                                                                      :     +- SubqueryAlias __auto_generated_subquery_name
!            :        +- Project [id#545, name#546]                                                                                                                                                                                 :        +- Distinct
!            :           +- Filter (id#545 = cast(1001 as int))                                                                                                                                                                     :           +- Project [id#545, name#546]
!            :              +- SubqueryAlias __auto_generated_subquery_name                                                                                                                                                         :              +- Filter (id#545 = cast(1001 as int))
!            :                 +- Aggregate [id#545, name#546], [id#545, name#546, max(salary#547) AS max(salary)#553]                                                                                                              :                 +- SubqueryAlias __auto_generated_subquery_name
!            :                    +- SubqueryAlias spark_catalog.default.employee                                                                                                                                                   :                    +- Aggregate [id#545, name#546], [id#545, name#546, max(salary#547) AS max(salary)#553]
!            :                       +- HiveTableRelation [`default`.`employee`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#545, name#546, salary#547, departmentid#548], Partition Cols: []]               :                       +- SubqueryAlias spark_catalog.default.employee
!            +- Project [id2#549, name2#550]                                                                                                                                                                                        :                          +- HiveTableRelation [`default`.`employee`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#545, name#546, salary#547, departmentid#548], Partition Cols: []]
!               +- SubqueryAlias spark_catalog.default.employee2                                                                                                                                                                    +- Project [id2#549, name2#550]
!                  +- HiveTableRelation [`default`.`employee2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id2#549, name2#550, salary#551], Partition Cols: []]                                                   +- Project [id2#549, name2#550]
!                                                                                                                                                                                                                                         +- SubqueryAlias spark_catalog.default.employee2
!                                                                                                                                                                                                                                            +- HiveTableRelation [`default`.`employee2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id2#549, name2#550, salary#551], Partition Cols: []]
           
21/09/05 23:02:38 WARN PlanChangeLogger: 
=== Applying Rule org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences ===
!'Distinct                                                                                                                                                                                                                 Distinct
!+- 'Project ['id AS staffid#542, 'name AS staffname#543, abc AS level#544]                                                                                                                                                +- Project [id#554 AS staffid#542, name#546 AS staffname#543, abc AS level#544]
!   +- 'Filter ('id = 1001)                                                                                                                                                                                                   +- Filter (id#554 = 1001)
       +- SubqueryAlias __auto_generated_subquery_name                                                                                                                                                                           +- SubqueryAlias __auto_generated_subquery_name
          +- Except false                                                                                                                                                                                                           +- Except false
             :- Project [cast(id#545 as string) AS id#554, name#546]                                                                                                                                                                   :- Project [cast(id#545 as string) AS id#554, name#546]
             :  +- Project [id#545, name#546]                                                                                                                                                                                          :  +- Project [id#545, name#546]
             :     +- SubqueryAlias __auto_generated_subquery_name                                                                                                                                                                     :     +- SubqueryAlias __auto_generated_subquery_name
             :        +- Distinct                                                                                                                                                                                                      :        +- Distinct
             :           +- Project [id#545, name#546]                                                                                                                                                                                 :           +- Project [id#545, name#546]
             :              +- Filter (id#545 = cast(1001 as int))                                                                                                                                                                     :              +- Filter (id#545 = cast(1001 as int))
             :                 +- SubqueryAlias __auto_generated_subquery_name                                                                                                                                                         :                 +- SubqueryAlias __auto_generated_subquery_name
             :                    +- Aggregate [id#545, name#546], [id#545, name#546, max(salary#547) AS max(salary)#553]                                                                                                              :                    +- Aggregate [id#545, name#546], [id#545, name#546, max(salary#547) AS max(salary)#553]
             :                       +- SubqueryAlias spark_catalog.default.employee                                                                                                                                                   :                       +- SubqueryAlias spark_catalog.default.employee
             :                          +- HiveTableRelation [`default`.`employee`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#545, name#546, salary#547, departmentid#548], Partition Cols: []]               :                          +- HiveTableRelation [`default`.`employee`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#545, name#546, salary#547, departmentid#548], Partition Cols: []]
             +- Project [id2#549, name2#550]                                                                                                                                                                                           +- Project [id2#549, name2#550]
                +- Project [id2#549, name2#550]                                                                                                                                                                                           +- Project [id2#549, name2#550]
                   +- SubqueryAlias spark_catalog.default.employee2                                                                                                                                                                          +- SubqueryAlias spark_catalog.default.employee2
                      +- HiveTableRelation [`default`.`employee2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id2#549, name2#550, salary#551], Partition Cols: []]                                                         +- HiveTableRelation [`default`.`employee2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id2#549, name2#550, salary#551], Partition Cols: []]
           
21/09/05 23:02:38 WARN PlanChangeLogger: 
=== Result of Batch Resolution ===
!'Distinct                                                                                                      Distinct
!+- 'Project ['id AS staffid#542, 'name AS staffname#543, abc AS level#544]                                     +- Project [id#554 AS staffid#542, name#546 AS staffname#543, abc AS level#544]
!   +- 'Filter ('id = 1001)                                                                                        +- Filter (id#554 = 1001)
!      +- 'SubqueryAlias __auto_generated_subquery_name                                                               +- SubqueryAlias __auto_generated_subquery_name
!         +- 'Except false                                                                                               +- Except false
!            :- 'Project ['id, 'name]                                                                                       :- Project [cast(id#545 as string) AS id#554, name#546]
!            :  +- 'SubqueryAlias __auto_generated_subquery_name                                                            :  +- Project [id#545, name#546]
!            :     +- 'Distinct                                                                                             :     +- SubqueryAlias __auto_generated_subquery_name
!            :        +- 'Project ['id, 'name]                                                                              :        +- Distinct
!            :           +- 'Filter ('id = 1001)                                                                            :           +- Project [id#545, name#546]
!            :              +- 'SubqueryAlias __auto_generated_subquery_name                                                :              +- Filter (id#545 = cast(1001 as int))
!            :                 +- 'Aggregate ['id, 'name], ['id, 'name, unresolvedalias('max('salary), None)]               :                 +- SubqueryAlias __auto_generated_subquery_name
!            :                    +- 'UnresolvedRelation [employee], [], false                                              :                    +- Aggregate [id#545, name#546], [id#545, name#546, max(salary#547) AS max(salary)#553]
!            +- 'Project ['id2, 'name2]                                                                                     :                       +- SubqueryAlias spark_catalog.default.employee
!               +- 'UnresolvedRelation [employee2], [], false                                                               :                          +- HiveTableRelation [`default`.`employee`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#545, name#546, salary#547, departmentid#548], Partition Cols: []]
!                                                                                                                           +- Project [id2#549, name2#550]
!                                                                                                                              +- Project [id2#549, name2#550]
!                                                                                                                                 +- SubqueryAlias spark_catalog.default.employee2
!                                                                                                                                    +- HiveTableRelation [`default`.`employee2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id2#549, name2#550, salary#551], Partition Cols: []]
          
21/09/05 23:02:38 WARN PlanChangeLogger: Batch Remove TempResolvedColumn has no effect.
21/09/05 23:02:38 WARN PlanChangeLogger: Batch Apply Char Padding has no effect.
21/09/05 23:02:38 WARN PlanChangeLogger: 
=== Applying Rule org.apache.spark.sql.hive.DetermineTableStats ===
 Distinct                                                                                                                                                                                                                  Distinct
 +- Project [id#554 AS staffid#542, name#546 AS staffname#543, abc AS level#544]                                                                                                                                           +- Project [id#554 AS staffid#542, name#546 AS staffname#543, abc AS level#544]
    +- Filter (id#554 = 1001)                                                                                                                                                                                                 +- Filter (id#554 = 1001)
       +- SubqueryAlias __auto_generated_subquery_name                                                                                                                                                                           +- SubqueryAlias __auto_generated_subquery_name
          +- Except false                                                                                                                                                                                                           +- Except false
             :- Project [cast(id#545 as string) AS id#554, name#546]                                                                                                                                                                   :- Project [cast(id#545 as string) AS id#554, name#546]
             :  +- Project [id#545, name#546]                                                                                                                                                                                          :  +- Project [id#545, name#546]
             :     +- SubqueryAlias __auto_generated_subquery_name                                                                                                                                                                     :     +- SubqueryAlias __auto_generated_subquery_name
             :        +- Distinct                                                                                                                                                                                                      :        +- Distinct
             :           +- Project [id#545, name#546]                                                                                                                                                                                 :           +- Project [id#545, name#546]
             :              +- Filter (id#545 = cast(1001 as int))                                                                                                                                                                     :              +- Filter (id#545 = cast(1001 as int))
             :                 +- SubqueryAlias __auto_generated_subquery_name                                                                                                                                                         :                 +- SubqueryAlias __auto_generated_subquery_name
             :                    +- Aggregate [id#545, name#546], [id#545, name#546, max(salary#547) AS max(salary)#553]                                                                                                              :                    +- Aggregate [id#545, name#546], [id#545, name#546, max(salary#547) AS max(salary)#553]
             :                       +- SubqueryAlias spark_catalog.default.employee                                                                                                                                                   :                       +- SubqueryAlias spark_catalog.default.employee
             :                          +- HiveTableRelation [`default`.`employee`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#545, name#546, salary#547, departmentid#548], Partition Cols: []]               :                          +- HiveTableRelation [`default`.`employee`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#545, name#546, salary#547, departmentid#548], Partition Cols: []]
             +- Project [id2#549, name2#550]                                                                                                                                                                                           +- Project [id2#549, name2#550]
                +- Project [id2#549, name2#550]                                                                                                                                                                                           +- Project [id2#549, name2#550]
                   +- SubqueryAlias spark_catalog.default.employee2                                                                                                                                                                          +- SubqueryAlias spark_catalog.default.employee2
                      +- HiveTableRelation [`default`.`employee2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id2#549, name2#550, salary#551], Partition Cols: []]                                                         +- HiveTableRelation [`default`.`employee2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id2#549, name2#550, salary#551], Partition Cols: []]
           
21/09/05 23:02:38 WARN PlanChangeLogger: 
=== Result of Batch Post-Hoc Resolution ===
 Distinct                                                                                                                                                                                                                  Distinct
 +- Project [id#554 AS staffid#542, name#546 AS staffname#543, abc AS level#544]                                                                                                                                           +- Project [id#554 AS staffid#542, name#546 AS staffname#543, abc AS level#544]
    +- Filter (id#554 = 1001)                                                                                                                                                                                                 +- Filter (id#554 = 1001)
       +- SubqueryAlias __auto_generated_subquery_name                                                                                                                                                                           +- SubqueryAlias __auto_generated_subquery_name
          +- Except false                                                                                                                                                                                                           +- Except false
             :- Project [cast(id#545 as string) AS id#554, name#546]                                                                                                                                                                   :- Project [cast(id#545 as string) AS id#554, name#546]
             :  +- Project [id#545, name#546]                                                                                                                                                                                          :  +- Project [id#545, name#546]
             :     +- SubqueryAlias __auto_generated_subquery_name                                                                                                                                                                     :     +- SubqueryAlias __auto_generated_subquery_name
             :        +- Distinct                                                                                                                                                                                                      :        +- Distinct
             :           +- Project [id#545, name#546]                                                                                                                                                                                 :           +- Project [id#545, name#546]
             :              +- Filter (id#545 = cast(1001 as int))                                                                                                                                                                     :              +- Filter (id#545 = cast(1001 as int))
             :                 +- SubqueryAlias __auto_generated_subquery_name                                                                                                                                                         :                 +- SubqueryAlias __auto_generated_subquery_name
             :                    +- Aggregate [id#545, name#546], [id#545, name#546, max(salary#547) AS max(salary)#553]                                                                                                              :                    +- Aggregate [id#545, name#546], [id#545, name#546, max(salary#547) AS max(salary)#553]
             :                       +- SubqueryAlias spark_catalog.default.employee                                                                                                                                                   :                       +- SubqueryAlias spark_catalog.default.employee
             :                          +- HiveTableRelation [`default`.`employee`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#545, name#546, salary#547, departmentid#548], Partition Cols: []]               :                          +- HiveTableRelation [`default`.`employee`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#545, name#546, salary#547, departmentid#548], Partition Cols: []]
             +- Project [id2#549, name2#550]                                                                                                                                                                                           +- Project [id2#549, name2#550]
                +- Project [id2#549, name2#550]                                                                                                                                                                                           +- Project [id2#549, name2#550]
                   +- SubqueryAlias spark_catalog.default.employee2                                                                                                                                                                          +- SubqueryAlias spark_catalog.default.employee2
                      +- HiveTableRelation [`default`.`employee2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id2#549, name2#550, salary#551], Partition Cols: []]                                                         +- HiveTableRelation [`default`.`employee2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id2#549, name2#550, salary#551], Partition Cols: []]
          
21/09/05 23:02:38 WARN PlanChangeLogger: Batch Remove Unresolved Hints has no effect.
21/09/05 23:02:38 WARN PlanChangeLogger: Batch Nondeterministic has no effect.
21/09/05 23:02:38 WARN PlanChangeLogger: Batch UDF has no effect.
21/09/05 23:02:38 WARN PlanChangeLogger: Batch UpdateNullability has no effect.
21/09/05 23:02:38 WARN PlanChangeLogger: Batch Subquery has no effect.
21/09/05 23:02:38 WARN PlanChangeLogger: 
=== Applying Rule org.apache.spark.sql.catalyst.analysis.CleanupAliases ===
 Distinct                                                                                                                                                                                                                  Distinct
 +- Project [id#554 AS staffid#542, name#546 AS staffname#543, abc AS level#544]                                                                                                                                           +- Project [id#554 AS staffid#542, name#546 AS staffname#543, abc AS level#544]
    +- Filter (id#554 = 1001)                                                                                                                                                                                                 +- Filter (id#554 = 1001)
       +- SubqueryAlias __auto_generated_subquery_name                                                                                                                                                                           +- SubqueryAlias __auto_generated_subquery_name
          +- Except false                                                                                                                                                                                                           +- Except false
             :- Project [cast(id#545 as string) AS id#554, name#546]                                                                                                                                                                   :- Project [cast(id#545 as string) AS id#554, name#546]
             :  +- Project [id#545, name#546]                                                                                                                                                                                          :  +- Project [id#545, name#546]
             :     +- SubqueryAlias __auto_generated_subquery_name                                                                                                                                                                     :     +- SubqueryAlias __auto_generated_subquery_name
             :        +- Distinct                                                                                                                                                                                                      :        +- Distinct
             :           +- Project [id#545, name#546]                                                                                                                                                                                 :           +- Project [id#545, name#546]
             :              +- Filter (id#545 = cast(1001 as int))                                                                                                                                                                     :              +- Filter (id#545 = cast(1001 as int))
             :                 +- SubqueryAlias __auto_generated_subquery_name                                                                                                                                                         :                 +- SubqueryAlias __auto_generated_subquery_name
             :                    +- Aggregate [id#545, name#546], [id#545, name#546, max(salary#547) AS max(salary)#553]                                                                                                              :                    +- Aggregate [id#545, name#546], [id#545, name#546, max(salary#547) AS max(salary)#553]
             :                       +- SubqueryAlias spark_catalog.default.employee                                                                                                                                                   :                       +- SubqueryAlias spark_catalog.default.employee
             :                          +- HiveTableRelation [`default`.`employee`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#545, name#546, salary#547, departmentid#548], Partition Cols: []]               :                          +- HiveTableRelation [`default`.`employee`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#545, name#546, salary#547, departmentid#548], Partition Cols: []]
             +- Project [id2#549, name2#550]                                                                                                                                                                                           +- Project [id2#549, name2#550]
                +- Project [id2#549, name2#550]                                                                                                                                                                                           +- Project [id2#549, name2#550]
                   +- SubqueryAlias spark_catalog.default.employee2                                                                                                                                                                          +- SubqueryAlias spark_catalog.default.employee2
                      +- HiveTableRelation [`default`.`employee2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id2#549, name2#550, salary#551], Partition Cols: []]                                                         +- HiveTableRelation [`default`.`employee2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id2#549, name2#550, salary#551], Partition Cols: []]
           
21/09/05 23:02:38 WARN PlanChangeLogger: 
=== Result of Batch Cleanup ===
 Distinct                                                                                                                                                                                                                  Distinct
 +- Project [id#554 AS staffid#542, name#546 AS staffname#543, abc AS level#544]                                                                                                                                           +- Project [id#554 AS staffid#542, name#546 AS staffname#543, abc AS level#544]
    +- Filter (id#554 = 1001)                                                                                                                                                                                                 +- Filter (id#554 = 1001)
       +- SubqueryAlias __auto_generated_subquery_name                                                                                                                                                                           +- SubqueryAlias __auto_generated_subquery_name
          +- Except false                                                                                                                                                                                                           +- Except false
             :- Project [cast(id#545 as string) AS id#554, name#546]                                                                                                                                                                   :- Project [cast(id#545 as string) AS id#554, name#546]
             :  +- Project [id#545, name#546]                                                                                                                                                                                          :  +- Project [id#545, name#546]
             :     +- SubqueryAlias __auto_generated_subquery_name                                                                                                                                                                     :     +- SubqueryAlias __auto_generated_subquery_name
             :        +- Distinct                                                                                                                                                                                                      :        +- Distinct
             :           +- Project [id#545, name#546]                                                                                                                                                                                 :           +- Project [id#545, name#546]
             :              +- Filter (id#545 = cast(1001 as int))                                                                                                                                                                     :              +- Filter (id#545 = cast(1001 as int))
             :                 +- SubqueryAlias __auto_generated_subquery_name                                                                                                                                                         :                 +- SubqueryAlias __auto_generated_subquery_name
             :                    +- Aggregate [id#545, name#546], [id#545, name#546, max(salary#547) AS max(salary)#553]                                                                                                              :                    +- Aggregate [id#545, name#546], [id#545, name#546, max(salary#547) AS max(salary)#553]
             :                       +- SubqueryAlias spark_catalog.default.employee                                                                                                                                                   :                       +- SubqueryAlias spark_catalog.default.employee
             :                          +- HiveTableRelation [`default`.`employee`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#545, name#546, salary#547, departmentid#548], Partition Cols: []]               :                          +- HiveTableRelation [`default`.`employee`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#545, name#546, salary#547, departmentid#548], Partition Cols: []]
             +- Project [id2#549, name2#550]                                                                                                                                                                                           +- Project [id2#549, name2#550]
                +- Project [id2#549, name2#550]                                                                                                                                                                                           +- Project [id2#549, name2#550]
                   +- SubqueryAlias spark_catalog.default.employee2                                                                                                                                                                          +- SubqueryAlias spark_catalog.default.employee2
                      +- HiveTableRelation [`default`.`employee2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id2#549, name2#550, salary#551], Partition Cols: []]                                                         +- HiveTableRelation [`default`.`employee2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id2#549, name2#550, salary#551], Partition Cols: []]
          
21/09/05 23:02:38 WARN PlanChangeLogger: Batch HandleAnalysisOnlyCommand has no effect.
21/09/05 23:02:38 WARN PlanChangeLogger: 
=== Metrics of Executed Rules ===
Total number of runs: 346
Total time: 0.230278017 seconds
Total number of effective runs: 13
Total time of effective runs: 0.187837601 seconds
      
21/09/05 23:02:38 WARN PlanChangeLogger: Batch Eliminate Distinct has no effect.
21/09/05 23:02:38 WARN PlanChangeLogger: 
=== Applying Rule org.apache.spark.sql.catalyst.analysis.EliminateSubqueryAliases ===
 Distinct                                                                                                                                                                                                                  Distinct
 +- Project [id#554 AS staffid#542, name#546 AS staffname#543, abc AS level#544]                                                                                                                                           +- Project [id#554 AS staffid#542, name#546 AS staffname#543, abc AS level#544]
    +- Filter (id#554 = 1001)                                                                                                                                                                                                 +- Filter (id#554 = 1001)
!      +- SubqueryAlias __auto_generated_subquery_name                                                                                                                                                                           +- Except false
!         +- Except false                                                                                                                                                                                                           :- Project [cast(id#545 as string) AS id#554, name#546]
!            :- Project [cast(id#545 as string) AS id#554, name#546]                                                                                                                                                                :  +- Project [id#545, name#546]
!            :  +- Project [id#545, name#546]                                                                                                                                                                                       :     +- Distinct
!            :     +- SubqueryAlias __auto_generated_subquery_name                                                                                                                                                                  :        +- Project [id#545, name#546]
!            :        +- Distinct                                                                                                                                                                                                   :           +- Filter (id#545 = cast(1001 as int))
!            :           +- Project [id#545, name#546]                                                                                                                                                                              :              +- Aggregate [id#545, name#546], [id#545, name#546, max(salary#547) AS max(salary)#553]
!            :              +- Filter (id#545 = cast(1001 as int))                                                                                                                                                                  :                 +- HiveTableRelation [`default`.`employee`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#545, name#546, salary#547, departmentid#548], Partition Cols: []]
!            :                 +- SubqueryAlias __auto_generated_subquery_name                                                                                                                                                      +- Project [id2#549, name2#550]
!            :                    +- Aggregate [id#545, name#546], [id#545, name#546, max(salary#547) AS max(salary)#553]                                                                                                              +- Project [id2#549, name2#550]
!            :                       +- SubqueryAlias spark_catalog.default.employee                                                                                                                                                      +- HiveTableRelation [`default`.`employee2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id2#549, name2#550, salary#551], Partition Cols: []]
!            :                          +- HiveTableRelation [`default`.`employee`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#545, name#546, salary#547, departmentid#548], Partition Cols: []]   
!            +- Project [id2#549, name2#550]                                                                                                                                                                               
!               +- Project [id2#549, name2#550]                                                                                                                                                                            
!                  +- SubqueryAlias spark_catalog.default.employee2                                                                                                                                                        
!                     +- HiveTableRelation [`default`.`employee2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id2#549, name2#550, salary#551], Partition Cols: []]                                    
           
21/09/05 23:02:38 WARN PlanChangeLogger: 
=== Result of Batch Finish Analysis ===
 Distinct                                                                                                                                                                                                                  Distinct
 +- Project [id#554 AS staffid#542, name#546 AS staffname#543, abc AS level#544]                                                                                                                                           +- Project [id#554 AS staffid#542, name#546 AS staffname#543, abc AS level#544]
    +- Filter (id#554 = 1001)                                                                                                                                                                                                 +- Filter (id#554 = 1001)
!      +- SubqueryAlias __auto_generated_subquery_name                                                                                                                                                                           +- Except false
!         +- Except false                                                                                                                                                                                                           :- Project [cast(id#545 as string) AS id#554, name#546]
!            :- Project [cast(id#545 as string) AS id#554, name#546]                                                                                                                                                                :  +- Project [id#545, name#546]
!            :  +- Project [id#545, name#546]                                                                                                                                                                                       :     +- Distinct
!            :     +- SubqueryAlias __auto_generated_subquery_name                                                                                                                                                                  :        +- Project [id#545, name#546]
!            :        +- Distinct                                                                                                                                                                                                   :           +- Filter (id#545 = cast(1001 as int))
!            :           +- Project [id#545, name#546]                                                                                                                                                                              :              +- Aggregate [id#545, name#546], [id#545, name#546, max(salary#547) AS max(salary)#553]
!            :              +- Filter (id#545 = cast(1001 as int))                                                                                                                                                                  :                 +- HiveTableRelation [`default`.`employee`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#545, name#546, salary#547, departmentid#548], Partition Cols: []]
!            :                 +- SubqueryAlias __auto_generated_subquery_name                                                                                                                                                      +- Project [id2#549, name2#550]
!            :                    +- Aggregate [id#545, name#546], [id#545, name#546, max(salary#547) AS max(salary)#553]                                                                                                              +- Project [id2#549, name2#550]
!            :                       +- SubqueryAlias spark_catalog.default.employee                                                                                                                                                      +- HiveTableRelation [`default`.`employee2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id2#549, name2#550, salary#551], Partition Cols: []]
!            :                          +- HiveTableRelation [`default`.`employee`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#545, name#546, salary#547, departmentid#548], Partition Cols: []]   
!            +- Project [id2#549, name2#550]                                                                                                                                                                               
!               +- Project [id2#549, name2#550]                                                                                                                                                                            
!                  +- SubqueryAlias spark_catalog.default.employee2                                                                                                                                                        
!                     +- HiveTableRelation [`default`.`employee2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id2#549, name2#550, salary#551], Partition Cols: []]                                    
          
21/09/05 23:02:38 WARN PlanChangeLogger: 
=== Applying Rule org.apache.spark.sql.catalyst.optimizer.RemoveNoopOperators ===
 Distinct                                                                                                                                                                                                      Distinct
 +- Project [id#554 AS staffid#542, name#546 AS staffname#543, abc AS level#544]                                                                                                                               +- Project [id#554 AS staffid#542, name#546 AS staffname#543, abc AS level#544]
    +- Filter (id#554 = 1001)                                                                                                                                                                                     +- Filter (id#554 = 1001)
       +- Except false                                                                                                                                                                                               +- Except false
          :- Project [cast(id#545 as string) AS id#554, name#546]                                                                                                                                                       :- Project [cast(id#545 as string) AS id#554, name#546]
!         :  +- Project [id#545, name#546]                                                                                                                                                                              :  +- Distinct
!         :     +- Distinct                                                                                                                                                                                             :     +- Project [id#545, name#546]
!         :        +- Project [id#545, name#546]                                                                                                                                                                        :        +- Filter (id#545 = cast(1001 as int))
!         :           +- Filter (id#545 = cast(1001 as int))                                                                                                                                                            :           +- Aggregate [id#545, name#546], [id#545, name#546, max(salary#547) AS max(salary)#553]
!         :              +- Aggregate [id#545, name#546], [id#545, name#546, max(salary#547) AS max(salary)#553]                                                                                                        :              +- HiveTableRelation [`default`.`employee`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#545, name#546, salary#547, departmentid#548], Partition Cols: []]
!         :                 +- HiveTableRelation [`default`.`employee`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#545, name#546, salary#547, departmentid#548], Partition Cols: []]            +- Project [id2#549, name2#550]
!         +- Project [id2#549, name2#550]                                                                                                                                                                                  +- HiveTableRelation [`default`.`employee2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id2#549, name2#550, salary#551], Partition Cols: []]
!            +- Project [id2#549, name2#550]                                                                                                                                                                   
!               +- HiveTableRelation [`default`.`employee2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id2#549, name2#550, salary#551], Partition Cols: []]                              
           
21/09/05 23:02:38 WARN PlanChangeLogger: 
=== Result of Batch Union ===
 Distinct                                                                                                                                                                                                      Distinct
 +- Project [id#554 AS staffid#542, name#546 AS staffname#543, abc AS level#544]                                                                                                                               +- Project [id#554 AS staffid#542, name#546 AS staffname#543, abc AS level#544]
    +- Filter (id#554 = 1001)                                                                                                                                                                                     +- Filter (id#554 = 1001)
       +- Except false                                                                                                                                                                                               +- Except false
          :- Project [cast(id#545 as string) AS id#554, name#546]                                                                                                                                                       :- Project [cast(id#545 as string) AS id#554, name#546]
!         :  +- Project [id#545, name#546]                                                                                                                                                                              :  +- Distinct
!         :     +- Distinct                                                                                                                                                                                             :     +- Project [id#545, name#546]
!         :        +- Project [id#545, name#546]                                                                                                                                                                        :        +- Filter (id#545 = cast(1001 as int))
!         :           +- Filter (id#545 = cast(1001 as int))                                                                                                                                                            :           +- Aggregate [id#545, name#546], [id#545, name#546, max(salary#547) AS max(salary)#553]
!         :              +- Aggregate [id#545, name#546], [id#545, name#546, max(salary#547) AS max(salary)#553]                                                                                                        :              +- HiveTableRelation [`default`.`employee`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#545, name#546, salary#547, departmentid#548], Partition Cols: []]
!         :                 +- HiveTableRelation [`default`.`employee`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#545, name#546, salary#547, departmentid#548], Partition Cols: []]            +- Project [id2#549, name2#550]
!         +- Project [id2#549, name2#550]                                                                                                                                                                                  +- HiveTableRelation [`default`.`employee2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id2#549, name2#550, salary#551], Partition Cols: []]
!            +- Project [id2#549, name2#550]                                                                                                                                                                   
!               +- HiveTableRelation [`default`.`employee2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id2#549, name2#550, salary#551], Partition Cols: []]                              
          
21/09/05 23:02:38 WARN PlanChangeLogger: Batch OptimizeLimitZero has no effect.
21/09/05 23:02:38 WARN PlanChangeLogger: Batch LocalRelation early has no effect.
21/09/05 23:02:38 WARN PlanChangeLogger: Batch Pullup Correlated Expressions has no effect.
21/09/05 23:02:38 WARN PlanChangeLogger: Batch Subquery has no effect.
21/09/05 23:02:38 WARN PlanChangeLogger: 
=== Applying Rule org.apache.spark.sql.catalyst.optimizer.ReplaceExceptWithAntiJoin ===
 Distinct                                                                                                                                                                                                   Distinct
 +- Project [id#554 AS staffid#542, name#546 AS staffname#543, abc AS level#544]                                                                                                                            +- Project [id#554 AS staffid#542, name#546 AS staffname#543, abc AS level#544]
    +- Filter (id#554 = 1001)                                                                                                                                                                                  +- Filter (id#554 = 1001)
!      +- Except false                                                                                                                                                                                            +- Distinct
!         :- Project [cast(id#545 as string) AS id#554, name#546]                                                                                                                                                    +- Join LeftAnti, ((id#554 <=> id2#549) AND (name#546 <=> name2#550))
!         :  +- Distinct                                                                                                                                                                                                :- Project [cast(id#545 as string) AS id#554, name#546]
!         :     +- Project [id#545, name#546]                                                                                                                                                                           :  +- Distinct
!         :        +- Filter (id#545 = cast(1001 as int))                                                                                                                                                               :     +- Project [id#545, name#546]
!         :           +- Aggregate [id#545, name#546], [id#545, name#546, max(salary#547) AS max(salary)#553]                                                                                                           :        +- Filter (id#545 = cast(1001 as int))
!         :              +- HiveTableRelation [`default`.`employee`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#545, name#546, salary#547, departmentid#548], Partition Cols: []]               :           +- Aggregate [id#545, name#546], [id#545, name#546, max(salary#547) AS max(salary)#553]
!         +- Project [id2#549, name2#550]                                                                                                                                                                               :              +- HiveTableRelation [`default`.`employee`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#545, name#546, salary#547, departmentid#548], Partition Cols: []]
!            +- HiveTableRelation [`default`.`employee2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id2#549, name2#550, salary#551], Partition Cols: []]                                          +- Project [id2#549, name2#550]
!                                                                                                                                                                                                                          +- HiveTableRelation [`default`.`employee2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id2#549, name2#550, salary#551], Partition Cols: []]
           
21/09/05 23:02:38 WARN PlanChangeLogger: 
=== Applying Rule org.apache.spark.sql.catalyst.optimizer.ReplaceDistinctWithAggregate ===
!Distinct                                                                                                                                                                                                      Aggregate [staffid#542, staffname#543, level#544], [staffid#542, staffname#543, level#544]
 +- Project [id#554 AS staffid#542, name#546 AS staffname#543, abc AS level#544]                                                                                                                               +- Project [id#554 AS staffid#542, name#546 AS staffname#543, abc AS level#544]
    +- Filter (id#554 = 1001)                                                                                                                                                                                     +- Filter (id#554 = 1001)
!      +- Distinct                                                                                                                                                                                                   +- Aggregate [id#554, name#546], [id#554, name#546]
          +- Join LeftAnti, ((id#554 <=> id2#549) AND (name#546 <=> name2#550))                                                                                                                                         +- Join LeftAnti, ((id#554 <=> id2#549) AND (name#546 <=> name2#550))
             :- Project [cast(id#545 as string) AS id#554, name#546]                                                                                                                                                       :- Project [cast(id#545 as string) AS id#554, name#546]
!            :  +- Distinct                                                                                                                                                                                                :  +- Aggregate [id#545, name#546], [id#545, name#546]
             :     +- Project [id#545, name#546]                                                                                                                                                                           :     +- Project [id#545, name#546]
             :        +- Filter (id#545 = cast(1001 as int))                                                                                                                                                               :        +- Filter (id#545 = cast(1001 as int))
             :           +- Aggregate [id#545, name#546], [id#545, name#546, max(salary#547) AS max(salary)#553]                                                                                                           :           +- Aggregate [id#545, name#546], [id#545, name#546, max(salary#547) AS max(salary)#553]
             :              +- HiveTableRelation [`default`.`employee`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#545, name#546, salary#547, departmentid#548], Partition Cols: []]               :              +- HiveTableRelation [`default`.`employee`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#545, name#546, salary#547, departmentid#548], Partition Cols: []]
             +- Project [id2#549, name2#550]                                                                                                                                                                               +- Project [id2#549, name2#550]
                +- HiveTableRelation [`default`.`employee2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id2#549, name2#550, salary#551], Partition Cols: []]                                             +- HiveTableRelation [`default`.`employee2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id2#549, name2#550, salary#551], Partition Cols: []]
           
21/09/05 23:02:38 WARN PlanChangeLogger: 
=== Result of Batch Replace Operators ===
!Distinct                                                                                                                                                                                                   Aggregate [staffid#542, staffname#543, level#544], [staffid#542, staffname#543, level#544]
 +- Project [id#554 AS staffid#542, name#546 AS staffname#543, abc AS level#544]                                                                                                                            +- Project [id#554 AS staffid#542, name#546 AS staffname#543, abc AS level#544]
    +- Filter (id#554 = 1001)                                                                                                                                                                                  +- Filter (id#554 = 1001)
!      +- Except false                                                                                                                                                                                            +- Aggregate [id#554, name#546], [id#554, name#546]
!         :- Project [cast(id#545 as string) AS id#554, name#546]                                                                                                                                                    +- Join LeftAnti, ((id#554 <=> id2#549) AND (name#546 <=> name2#550))
!         :  +- Distinct                                                                                                                                                                                                :- Project [cast(id#545 as string) AS id#554, name#546]
!         :     +- Project [id#545, name#546]                                                                                                                                                                           :  +- Aggregate [id#545, name#546], [id#545, name#546]
!         :        +- Filter (id#545 = cast(1001 as int))                                                                                                                                                               :     +- Project [id#545, name#546]
!         :           +- Aggregate [id#545, name#546], [id#545, name#546, max(salary#547) AS max(salary)#553]                                                                                                           :        +- Filter (id#545 = cast(1001 as int))
!         :              +- HiveTableRelation [`default`.`employee`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#545, name#546, salary#547, departmentid#548], Partition Cols: []]               :           +- Aggregate [id#545, name#546], [id#545, name#546, max(salary#547) AS max(salary)#553]
!         +- Project [id2#549, name2#550]                                                                                                                                                                               :              +- HiveTableRelation [`default`.`employee`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#545, name#546, salary#547, departmentid#548], Partition Cols: []]
!            +- HiveTableRelation [`default`.`employee2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id2#549, name2#550, salary#551], Partition Cols: []]                                          +- Project [id2#549, name2#550]
!                                                                                                                                                                                                                          +- HiveTableRelation [`default`.`employee2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id2#549, name2#550, salary#551], Partition Cols: []]
          
21/09/05 23:02:38 WARN PlanChangeLogger: Batch Aggregate has no effect.
21/09/05 23:02:38 WARN PlanChangeLogger: 
=== Applying Rule org.apache.spark.sql.catalyst.optimizer.PushDownPredicates ===
 Aggregate [staffid#542, staffname#543, level#544], [staffid#542, staffname#543, level#544]                                                                                                                    Aggregate [staffid#542, staffname#543, level#544], [staffid#542, staffname#543, level#544]
 +- Project [id#554 AS staffid#542, name#546 AS staffname#543, abc AS level#544]                                                                                                                               +- Project [id#554 AS staffid#542, name#546 AS staffname#543, abc AS level#544]
!   +- Filter (id#554 = 1001)                                                                                                                                                                                     +- Aggregate [id#554, name#546], [id#554, name#546]
!      +- Aggregate [id#554, name#546], [id#554, name#546]                                                                                                                                                           +- Join LeftAnti, ((id#554 <=> id2#549) AND (name#546 <=> name2#550))
!         +- Join LeftAnti, ((id#554 <=> id2#549) AND (name#546 <=> name2#550))                                                                                                                                         :- Project [cast(id#545 as string) AS id#554, name#546]
!            :- Project [cast(id#545 as string) AS id#554, name#546]                                                                                                                                                    :  +- Aggregate [id#545, name#546], [id#545, name#546]
!            :  +- Aggregate [id#545, name#546], [id#545, name#546]                                                                                                                                                     :     +- Project [id#545, name#546]
!            :     +- Project [id#545, name#546]                                                                                                                                                                        :        +- Filter ((id#545 = cast(1001 as int)) AND (cast(id#545 as string) = 1001))
!            :        +- Filter (id#545 = cast(1001 as int))                                                                                                                                                            :           +- Aggregate [id#545, name#546], [id#545, name#546, max(salary#547) AS max(salary)#553]
!            :           +- Aggregate [id#545, name#546], [id#545, name#546, max(salary#547) AS max(salary)#553]                                                                                                        :              +- HiveTableRelation [`default`.`employee`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#545, name#546, salary#547, departmentid#548], Partition Cols: []]
!            :              +- HiveTableRelation [`default`.`employee`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#545, name#546, salary#547, departmentid#548], Partition Cols: []]            +- Project [id2#549, name2#550]
!            +- Project [id2#549, name2#550]                                                                                                                                                                               +- HiveTableRelation [`default`.`employee2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id2#549, name2#550, salary#551], Partition Cols: []]
!               +- HiveTableRelation [`default`.`employee2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id2#549, name2#550, salary#551], Partition Cols: []]                              
           
21/09/05 23:02:38 WARN PlanChangeLogger: 
=== Applying Rule org.apache.spark.sql.catalyst.optimizer.PushDownLeftSemiAntiJoin ===
 Aggregate [staffid#542, staffname#543, level#544], [staffid#542, staffname#543, level#544]                                                                                                                 Aggregate [staffid#542, staffname#543, level#544], [staffid#542, staffname#543, level#544]
 +- Project [id#554 AS staffid#542, name#546 AS staffname#543, abc AS level#544]                                                                                                                            +- Project [id#554 AS staffid#542, name#546 AS staffname#543, abc AS level#544]
    +- Aggregate [id#554, name#546], [id#554, name#546]                                                                                                                                                        +- Aggregate [id#554, name#546], [id#554, name#546]
!      +- Join LeftAnti, ((id#554 <=> id2#549) AND (name#546 <=> name2#550))                                                                                                                                      +- Project [cast(id#545 as string) AS id#554, name#546]
!         :- Project [cast(id#545 as string) AS id#554, name#546]                                                                                                                                                    +- Join LeftAnti, ((cast(id#545 as string) <=> id2#549) AND (name#546 <=> name2#550))
!         :  +- Aggregate [id#545, name#546], [id#545, name#546]                                                                                                                                                        :- Aggregate [id#545, name#546], [id#545, name#546]
!         :     +- Project [id#545, name#546]                                                                                                                                                                           :  +- Project [id#545, name#546]
!         :        +- Filter ((id#545 = cast(1001 as int)) AND (cast(id#545 as string) = 1001))                                                                                                                         :     +- Filter ((id#545 = cast(1001 as int)) AND (cast(id#545 as string) = 1001))
!         :           +- Aggregate [id#545, name#546], [id#545, name#546, max(salary#547) AS max(salary)#553]                                                                                                           :        +- Aggregate [id#545, name#546], [id#545, name#546, max(salary#547) AS max(salary)#553]
!         :              +- HiveTableRelation [`default`.`employee`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#545, name#546, salary#547, departmentid#548], Partition Cols: []]               :           +- HiveTableRelation [`default`.`employee`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#545, name#546, salary#547, departmentid#548], Partition Cols: []]
!         +- Project [id2#549, name2#550]                                                                                                                                                                               +- Project [id2#549, name2#550]
!            +- HiveTableRelation [`default`.`employee2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id2#549, name2#550, salary#551], Partition Cols: []]                                             +- HiveTableRelation [`default`.`employee2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id2#549, name2#550, salary#551], Partition Cols: []]
           
21/09/05 23:02:38 WARN PlanChangeLogger: 
=== Applying Rule org.apache.spark.sql.catalyst.optimizer.ColumnPruning ===
 Aggregate [staffid#542, staffname#543, level#544], [staffid#542, staffname#543, level#544]                                                                                                                 Aggregate [staffid#542, staffname#543, level#544], [staffid#542, staffname#543, level#544]
 +- Project [id#554 AS staffid#542, name#546 AS staffname#543, abc AS level#544]                                                                                                                            +- Project [id#554 AS staffid#542, name#546 AS staffname#543, abc AS level#544]
    +- Aggregate [id#554, name#546], [id#554, name#546]                                                                                                                                                        +- Aggregate [id#554, name#546], [id#554, name#546]
       +- Project [cast(id#545 as string) AS id#554, name#546]                                                                                                                                                    +- Project [cast(id#545 as string) AS id#554, name#546]
          +- Join LeftAnti, ((cast(id#545 as string) <=> id2#549) AND (name#546 <=> name2#550))                                                                                                                      +- Join LeftAnti, ((cast(id#545 as string) <=> id2#549) AND (name#546 <=> name2#550))
             :- Aggregate [id#545, name#546], [id#545, name#546]                                                                                                                                                        :- Aggregate [id#545, name#546], [id#545, name#546]
             :  +- Project [id#545, name#546]                                                                                                                                                                           :  +- Project [id#545, name#546]
             :     +- Filter ((id#545 = cast(1001 as int)) AND (cast(id#545 as string) = 1001))                                                                                                                         :     +- Filter ((id#545 = cast(1001 as int)) AND (cast(id#545 as string) = 1001))
!            :        +- Aggregate [id#545, name#546], [id#545, name#546, max(salary#547) AS max(salary)#553]                                                                                                           :        +- Aggregate [id#545, name#546], [id#545, name#546]
!            :           +- HiveTableRelation [`default`.`employee`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#545, name#546, salary#547, departmentid#548], Partition Cols: []]               :           +- Project [id#545, name#546]
!            +- Project [id2#549, name2#550]                                                                                                                                                                            :              +- HiveTableRelation [`default`.`employee`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#545, name#546, salary#547, departmentid#548], Partition Cols: []]
!               +- HiveTableRelation [`default`.`employee2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id2#549, name2#550, salary#551], Partition Cols: []]                                       +- Project [id2#549, name2#550]
!                                                                                                                                                                                                                          +- HiveTableRelation [`default`.`employee2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id2#549, name2#550, salary#551], Partition Cols: []]
           
21/09/05 23:02:38 WARN PlanChangeLogger: 
=== Applying Rule org.apache.spark.sql.catalyst.optimizer.CollapseProject ===
 Aggregate [staffid#542, staffname#543, level#544], [staffid#542, staffname#543, level#544]                                                                                                                    Aggregate [staffid#542, staffname#543, level#544], [staffid#542, staffname#543, level#544]
!+- Project [id#554 AS staffid#542, name#546 AS staffname#543, abc AS level#544]                                                                                                                               +- Aggregate [id#554, name#546], [id#554 AS staffid#542, name#546 AS staffname#543, abc AS level#544]
!   +- Aggregate [id#554, name#546], [id#554, name#546]                                                                                                                                                           +- Project [cast(id#545 as string) AS id#554, name#546]
!      +- Project [cast(id#545 as string) AS id#554, name#546]                                                                                                                                                       +- Join LeftAnti, ((cast(id#545 as string) <=> id2#549) AND (name#546 <=> name2#550))
!         +- Join LeftAnti, ((cast(id#545 as string) <=> id2#549) AND (name#546 <=> name2#550))                                                                                                                         :- Aggregate [id#545, name#546], [id#545, name#546]
!            :- Aggregate [id#545, name#546], [id#545, name#546]                                                                                                                                                        :  +- Project [id#545, name#546]
!            :  +- Project [id#545, name#546]                                                                                                                                                                           :     +- Filter ((id#545 = cast(1001 as int)) AND (cast(id#545 as string) = 1001))
!            :     +- Filter ((id#545 = cast(1001 as int)) AND (cast(id#545 as string) = 1001))                                                                                                                         :        +- Aggregate [id#545, name#546], [id#545, name#546]
!            :        +- Aggregate [id#545, name#546], [id#545, name#546]                                                                                                                                               :           +- Project [id#545, name#546]
!            :           +- Project [id#545, name#546]                                                                                                                                                                  :              +- HiveTableRelation [`default`.`employee`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#545, name#546, salary#547, departmentid#548], Partition Cols: []]
!            :              +- HiveTableRelation [`default`.`employee`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#545, name#546, salary#547, departmentid#548], Partition Cols: []]            +- Project [id2#549, name2#550]
!            +- Project [id2#549, name2#550]                                                                                                                                                                               +- HiveTableRelation [`default`.`employee2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id2#549, name2#550, salary#551], Partition Cols: []]
!               +- HiveTableRelation [`default`.`employee2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id2#549, name2#550, salary#551], Partition Cols: []]                              
           
21/09/05 23:02:38 WARN PlanChangeLogger: 
=== Applying Rule org.apache.spark.sql.catalyst.optimizer.FoldablePropagation ===
!Aggregate [staffid#542, staffname#543, level#544], [staffid#542, staffname#543, level#544]                                                                                                                 Aggregate [staffid#542, staffname#543, abc], [staffid#542, staffname#543, abc AS level#544]
 +- Aggregate [id#554, name#546], [id#554 AS staffid#542, name#546 AS staffname#543, abc AS level#544]                                                                                                      +- Aggregate [id#554, name#546], [id#554 AS staffid#542, name#546 AS staffname#543, abc AS level#544]
    +- Project [cast(id#545 as string) AS id#554, name#546]                                                                                                                                                    +- Project [cast(id#545 as string) AS id#554, name#546]
       +- Join LeftAnti, ((cast(id#545 as string) <=> id2#549) AND (name#546 <=> name2#550))                                                                                                                      +- Join LeftAnti, ((cast(id#545 as string) <=> id2#549) AND (name#546 <=> name2#550))
          :- Aggregate [id#545, name#546], [id#545, name#546]                                                                                                                                                        :- Aggregate [id#545, name#546], [id#545, name#546]
          :  +- Project [id#545, name#546]                                                                                                                                                                           :  +- Project [id#545, name#546]
          :     +- Filter ((id#545 = cast(1001 as int)) AND (cast(id#545 as string) = 1001))                                                                                                                         :     +- Filter ((id#545 = cast(1001 as int)) AND (cast(id#545 as string) = 1001))
          :        +- Aggregate [id#545, name#546], [id#545, name#546]                                                                                                                                               :        +- Aggregate [id#545, name#546], [id#545, name#546]
          :           +- Project [id#545, name#546]                                                                                                                                                                  :           +- Project [id#545, name#546]
          :              +- HiveTableRelation [`default`.`employee`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#545, name#546, salary#547, departmentid#548], Partition Cols: []]            :              +- HiveTableRelation [`default`.`employee`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#545, name#546, salary#547, departmentid#548], Partition Cols: []]
          +- Project [id2#549, name2#550]                                                                                                                                                                            +- Project [id2#549, name2#550]
             +- HiveTableRelation [`default`.`employee2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id2#549, name2#550, salary#551], Partition Cols: []]                                          +- HiveTableRelation [`default`.`employee2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id2#549, name2#550, salary#551], Partition Cols: []]
           
21/09/05 23:02:38 WARN PlanChangeLogger: 
=== Applying Rule org.apache.spark.sql.catalyst.optimizer.ConstantFolding ===
 Aggregate [staffid#542, staffname#543, abc], [staffid#542, staffname#543, abc AS level#544]                                                                                                                Aggregate [staffid#542, staffname#543, abc], [staffid#542, staffname#543, abc AS level#544]
 +- Aggregate [id#554, name#546], [id#554 AS staffid#542, name#546 AS staffname#543, abc AS level#544]                                                                                                      +- Aggregate [id#554, name#546], [id#554 AS staffid#542, name#546 AS staffname#543, abc AS level#544]
    +- Project [cast(id#545 as string) AS id#554, name#546]                                                                                                                                                    +- Project [cast(id#545 as string) AS id#554, name#546]
       +- Join LeftAnti, ((cast(id#545 as string) <=> id2#549) AND (name#546 <=> name2#550))                                                                                                                      +- Join LeftAnti, ((cast(id#545 as string) <=> id2#549) AND (name#546 <=> name2#550))
          :- Aggregate [id#545, name#546], [id#545, name#546]                                                                                                                                                        :- Aggregate [id#545, name#546], [id#545, name#546]
          :  +- Project [id#545, name#546]                                                                                                                                                                           :  +- Project [id#545, name#546]
!         :     +- Filter ((id#545 = cast(1001 as int)) AND (cast(id#545 as string) = 1001))                                                                                                                         :     +- Filter ((id#545 = 1001) AND (cast(id#545 as string) = 1001))
          :        +- Aggregate [id#545, name#546], [id#545, name#546]                                                                                                                                               :        +- Aggregate [id#545, name#546], [id#545, name#546]
          :           +- Project [id#545, name#546]                                                                                                                                                                  :           +- Project [id#545, name#546]
          :              +- HiveTableRelation [`default`.`employee`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#545, name#546, salary#547, departmentid#548], Partition Cols: []]            :              +- HiveTableRelation [`default`.`employee`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#545, name#546, salary#547, departmentid#548], Partition Cols: []]
          +- Project [id2#549, name2#550]                                                                                                                                                                            +- Project [id2#549, name2#550]
             +- HiveTableRelation [`default`.`employee2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id2#549, name2#550, salary#551], Partition Cols: []]                                          +- HiveTableRelation [`default`.`employee2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id2#549, name2#550, salary#551], Partition Cols: []]
           
21/09/05 23:02:38 WARN PlanChangeLogger: 
=== Applying Rule org.apache.spark.sql.catalyst.optimizer.RemoveRedundantAggregates ===
!Aggregate [staffid#542, staffname#543, abc], [staffid#542, staffname#543, abc AS level#544]                                                                                                                Aggregate [id#554, name#546, abc], [id#554 AS staffid#542, name#546 AS staffname#543, abc AS level#544]
!+- Aggregate [id#554, name#546], [id#554 AS staffid#542, name#546 AS staffname#543, abc AS level#544]                                                                                                      +- Project [cast(id#545 as string) AS id#554, name#546]
!   +- Project [cast(id#545 as string) AS id#554, name#546]                                                                                                                                                    +- Join LeftAnti, ((cast(id#545 as string) <=> id2#549) AND (name#546 <=> name2#550))
!      +- Join LeftAnti, ((cast(id#545 as string) <=> id2#549) AND (name#546 <=> name2#550))                                                                                                                      :- Aggregate [id#545, name#546], [id#545, name#546]
!         :- Aggregate [id#545, name#546], [id#545, name#546]                                                                                                                                                     :  +- Project [id#545, name#546]
!         :  +- Project [id#545, name#546]                                                                                                                                                                        :     +- Filter ((id#545 = 1001) AND (cast(id#545 as string) = 1001))
!         :     +- Filter ((id#545 = 1001) AND (cast(id#545 as string) = 1001))                                                                                                                                   :        +- Aggregate [id#545, name#546], [id#545, name#546]
!         :        +- Aggregate [id#545, name#546], [id#545, name#546]                                                                                                                                            :           +- Project [id#545, name#546]
!         :           +- Project [id#545, name#546]                                                                                                                                                               :              +- HiveTableRelation [`default`.`employee`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#545, name#546, salary#547, departmentid#548], Partition Cols: []]
!         :              +- HiveTableRelation [`default`.`employee`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#545, name#546, salary#547, departmentid#548], Partition Cols: []]         +- Project [id2#549, name2#550]
!         +- Project [id2#549, name2#550]                                                                                                                                                                            +- HiveTableRelation [`default`.`employee2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id2#549, name2#550, salary#551], Partition Cols: []]
!            +- HiveTableRelation [`default`.`employee2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id2#549, name2#550, salary#551], Partition Cols: []]                              
           
21/09/05 23:02:38 WARN PlanChangeLogger: 
=== Applying Rule org.apache.spark.sql.catalyst.optimizer.RemoveNoopOperators ===
 Aggregate [id#554, name#546, abc], [id#554 AS staffid#542, name#546 AS staffname#543, abc AS level#544]                                                                                                 Aggregate [id#554, name#546, abc], [id#554 AS staffid#542, name#546 AS staffname#543, abc AS level#544]
 +- Project [cast(id#545 as string) AS id#554, name#546]                                                                                                                                                 +- Project [cast(id#545 as string) AS id#554, name#546]
    +- Join LeftAnti, ((cast(id#545 as string) <=> id2#549) AND (name#546 <=> name2#550))                                                                                                                   +- Join LeftAnti, ((cast(id#545 as string) <=> id2#549) AND (name#546 <=> name2#550))
       :- Aggregate [id#545, name#546], [id#545, name#546]                                                                                                                                                     :- Aggregate [id#545, name#546], [id#545, name#546]
!      :  +- Project [id#545, name#546]                                                                                                                                                                        :  +- Filter ((id#545 = 1001) AND (cast(id#545 as string) = 1001))
!      :     +- Filter ((id#545 = 1001) AND (cast(id#545 as string) = 1001))                                                                                                                                   :     +- Aggregate [id#545, name#546], [id#545, name#546]
!      :        +- Aggregate [id#545, name#546], [id#545, name#546]                                                                                                                                            :        +- Project [id#545, name#546]
!      :           +- Project [id#545, name#546]                                                                                                                                                               :           +- HiveTableRelation [`default`.`employee`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#545, name#546, salary#547, departmentid#548], Partition Cols: []]
!      :              +- HiveTableRelation [`default`.`employee`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#545, name#546, salary#547, departmentid#548], Partition Cols: []]         +- Project [id2#549, name2#550]
!      +- Project [id2#549, name2#550]                                                                                                                                                                            +- HiveTableRelation [`default`.`employee2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id2#549, name2#550, salary#551], Partition Cols: []]
!         +- HiveTableRelation [`default`.`employee2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id2#549, name2#550, salary#551], Partition Cols: []]                              
           
21/09/05 23:02:38 WARN PlanChangeLogger: 
=== Applying Rule org.apache.spark.sql.catalyst.optimizer.PushDownPredicates ===
 Aggregate [id#554, name#546, abc], [id#554 AS staffid#542, name#546 AS staffname#543, abc AS level#544]                                                                                              Aggregate [id#554, name#546, abc], [id#554 AS staffid#542, name#546 AS staffname#543, abc AS level#544]
 +- Project [cast(id#545 as string) AS id#554, name#546]                                                                                                                                              +- Project [cast(id#545 as string) AS id#554, name#546]
    +- Join LeftAnti, ((cast(id#545 as string) <=> id2#549) AND (name#546 <=> name2#550))                                                                                                                +- Join LeftAnti, ((cast(id#545 as string) <=> id2#549) AND (name#546 <=> name2#550))
       :- Aggregate [id#545, name#546], [id#545, name#546]                                                                                                                                                  :- Aggregate [id#545, name#546], [id#545, name#546]
!      :  +- Filter ((id#545 = 1001) AND (cast(id#545 as string) = 1001))                                                                                                                                   :  +- Aggregate [id#545, name#546], [id#545, name#546]
!      :     +- Aggregate [id#545, name#546], [id#545, name#546]                                                                                                                                            :     +- Project [id#545, name#546]
!      :        +- Project [id#545, name#546]                                                                                                                                                               :        +- Filter ((id#545 = 1001) AND (cast(id#545 as string) = 1001))
       :           +- HiveTableRelation [`default`.`employee`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#545, name#546, salary#547, departmentid#548], Partition Cols: []]         :           +- HiveTableRelation [`default`.`employee`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#545, name#546, salary#547, departmentid#548], Partition Cols: []]
       +- Project [id2#549, name2#550]                                                                                                                                                                      +- Project [id2#549, name2#550]
          +- HiveTableRelation [`default`.`employee2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id2#549, name2#550, salary#551], Partition Cols: []]                                    +- HiveTableRelation [`default`.`employee2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id2#549, name2#550, salary#551], Partition Cols: []]
           
21/09/05 23:02:38 WARN PlanChangeLogger: 
=== Applying Rule org.apache.spark.sql.catalyst.optimizer.ConstantPropagation ===
 Aggregate [id#554, name#546, abc], [id#554 AS staffid#542, name#546 AS staffname#543, abc AS level#544]                                                                                              Aggregate [id#554, name#546, abc], [id#554 AS staffid#542, name#546 AS staffname#543, abc AS level#544]
 +- Project [cast(id#545 as string) AS id#554, name#546]                                                                                                                                              +- Project [cast(id#545 as string) AS id#554, name#546]
    +- Join LeftAnti, ((cast(id#545 as string) <=> id2#549) AND (name#546 <=> name2#550))                                                                                                                +- Join LeftAnti, ((cast(id#545 as string) <=> id2#549) AND (name#546 <=> name2#550))
       :- Aggregate [id#545, name#546], [id#545, name#546]                                                                                                                                                  :- Aggregate [id#545, name#546], [id#545, name#546]
       :  +- Aggregate [id#545, name#546], [id#545, name#546]                                                                                                                                               :  +- Aggregate [id#545, name#546], [id#545, name#546]
       :     +- Project [id#545, name#546]                                                                                                                                                                  :     +- Project [id#545, name#546]
!      :        +- Filter ((id#545 = 1001) AND (cast(id#545 as string) = 1001))                                                                                                                             :        +- Filter ((id#545 = 1001) AND (cast(1001 as string) = 1001))
       :           +- HiveTableRelation [`default`.`employee`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#545, name#546, salary#547, departmentid#548], Partition Cols: []]         :           +- HiveTableRelation [`default`.`employee`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#545, name#546, salary#547, departmentid#548], Partition Cols: []]
       +- Project [id2#549, name2#550]                                                                                                                                                                      +- Project [id2#549, name2#550]
          +- HiveTableRelation [`default`.`employee2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id2#549, name2#550, salary#551], Partition Cols: []]                                    +- HiveTableRelation [`default`.`employee2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id2#549, name2#550, salary#551], Partition Cols: []]
           
21/09/05 23:02:38 WARN PlanChangeLogger: 
=== Applying Rule org.apache.spark.sql.catalyst.optimizer.ConstantFolding ===
 Aggregate [id#554, name#546, abc], [id#554 AS staffid#542, name#546 AS staffname#543, abc AS level#544]                                                                                              Aggregate [id#554, name#546, abc], [id#554 AS staffid#542, name#546 AS staffname#543, abc AS level#544]
 +- Project [cast(id#545 as string) AS id#554, name#546]                                                                                                                                              +- Project [cast(id#545 as string) AS id#554, name#546]
    +- Join LeftAnti, ((cast(id#545 as string) <=> id2#549) AND (name#546 <=> name2#550))                                                                                                                +- Join LeftAnti, ((cast(id#545 as string) <=> id2#549) AND (name#546 <=> name2#550))
       :- Aggregate [id#545, name#546], [id#545, name#546]                                                                                                                                                  :- Aggregate [id#545, name#546], [id#545, name#546]
       :  +- Aggregate [id#545, name#546], [id#545, name#546]                                                                                                                                               :  +- Aggregate [id#545, name#546], [id#545, name#546]
       :     +- Project [id#545, name#546]                                                                                                                                                                  :     +- Project [id#545, name#546]
!      :        +- Filter ((id#545 = 1001) AND (cast(1001 as string) = 1001))                                                                                                                               :        +- Filter ((id#545 = 1001) AND true)
       :           +- HiveTableRelation [`default`.`employee`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#545, name#546, salary#547, departmentid#548], Partition Cols: []]         :           +- HiveTableRelation [`default`.`employee`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#545, name#546, salary#547, departmentid#548], Partition Cols: []]
       +- Project [id2#549, name2#550]                                                                                                                                                                      +- Project [id2#549, name2#550]
          +- HiveTableRelation [`default`.`employee2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id2#549, name2#550, salary#551], Partition Cols: []]                                    +- HiveTableRelation [`default`.`employee2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id2#549, name2#550, salary#551], Partition Cols: []]
           
21/09/05 23:02:38 WARN PlanChangeLogger: 
=== Applying Rule org.apache.spark.sql.catalyst.optimizer.BooleanSimplification ===
 Aggregate [id#554, name#546, abc], [id#554 AS staffid#542, name#546 AS staffname#543, abc AS level#544]                                                                                              Aggregate [id#554, name#546, abc], [id#554 AS staffid#542, name#546 AS staffname#543, abc AS level#544]
 +- Project [cast(id#545 as string) AS id#554, name#546]                                                                                                                                              +- Project [cast(id#545 as string) AS id#554, name#546]
    +- Join LeftAnti, ((cast(id#545 as string) <=> id2#549) AND (name#546 <=> name2#550))                                                                                                                +- Join LeftAnti, ((cast(id#545 as string) <=> id2#549) AND (name#546 <=> name2#550))
       :- Aggregate [id#545, name#546], [id#545, name#546]                                                                                                                                                  :- Aggregate [id#545, name#546], [id#545, name#546]
       :  +- Aggregate [id#545, name#546], [id#545, name#546]                                                                                                                                               :  +- Aggregate [id#545, name#546], [id#545, name#546]
       :     +- Project [id#545, name#546]                                                                                                                                                                  :     +- Project [id#545, name#546]
!      :        +- Filter ((id#545 = 1001) AND true)                                                                                                                                                        :        +- Filter (id#545 = 1001)
       :           +- HiveTableRelation [`default`.`employee`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#545, name#546, salary#547, departmentid#548], Partition Cols: []]         :           +- HiveTableRelation [`default`.`employee`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#545, name#546, salary#547, departmentid#548], Partition Cols: []]
       +- Project [id2#549, name2#550]                                                                                                                                                                      +- Project [id2#549, name2#550]
          +- HiveTableRelation [`default`.`employee2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id2#549, name2#550, salary#551], Partition Cols: []]                                    +- HiveTableRelation [`default`.`employee2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id2#549, name2#550, salary#551], Partition Cols: []]
           
21/09/05 23:02:38 WARN PlanChangeLogger: 
=== Applying Rule org.apache.spark.sql.catalyst.optimizer.RemoveRedundantAggregates ===
 Aggregate [id#554, name#546, abc], [id#554 AS staffid#542, name#546 AS staffname#543, abc AS level#544]                                                                                              Aggregate [id#554, name#546, abc], [id#554 AS staffid#542, name#546 AS staffname#543, abc AS level#544]
 +- Project [cast(id#545 as string) AS id#554, name#546]                                                                                                                                              +- Project [cast(id#545 as string) AS id#554, name#546]
    +- Join LeftAnti, ((cast(id#545 as string) <=> id2#549) AND (name#546 <=> name2#550))                                                                                                                +- Join LeftAnti, ((cast(id#545 as string) <=> id2#549) AND (name#546 <=> name2#550))
       :- Aggregate [id#545, name#546], [id#545, name#546]                                                                                                                                                  :- Aggregate [id#545, name#546], [id#545, name#546]
!      :  +- Aggregate [id#545, name#546], [id#545, name#546]                                                                                                                                               :  +- Project [id#545, name#546]
!      :     +- Project [id#545, name#546]                                                                                                                                                                  :     +- Filter (id#545 = 1001)
!      :        +- Filter (id#545 = 1001)                                                                                                                                                                   :        +- HiveTableRelation [`default`.`employee`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#545, name#546, salary#547, departmentid#548], Partition Cols: []]
!      :           +- HiveTableRelation [`default`.`employee`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#545, name#546, salary#547, departmentid#548], Partition Cols: []]         +- Project [id2#549, name2#550]
!      +- Project [id2#549, name2#550]                                                                                                                                                                         +- HiveTableRelation [`default`.`employee2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id2#549, name2#550, salary#551], Partition Cols: []]
!         +- HiveTableRelation [`default`.`employee2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id2#549, name2#550, salary#551], Partition Cols: []]                           
           
21/09/05 23:02:38 WARN PlanChangeLogger: 
=== Result of Batch Operator Optimization before Inferring Filters ===
!Aggregate [staffid#542, staffname#543, level#544], [staffid#542, staffname#543, level#544]                                                                                                                    Aggregate [id#554, name#546, abc], [id#554 AS staffid#542, name#546 AS staffname#543, abc AS level#544]
!+- Project [id#554 AS staffid#542, name#546 AS staffname#543, abc AS level#544]                                                                                                                               +- Project [cast(id#545 as string) AS id#554, name#546]
!   +- Filter (id#554 = 1001)                                                                                                                                                                                     +- Join LeftAnti, ((cast(id#545 as string) <=> id2#549) AND (name#546 <=> name2#550))
!      +- Aggregate [id#554, name#546], [id#554, name#546]                                                                                                                                                           :- Aggregate [id#545, name#546], [id#545, name#546]
!         +- Join LeftAnti, ((id#554 <=> id2#549) AND (name#546 <=> name2#550))                                                                                                                                      :  +- Project [id#545, name#546]
!            :- Project [cast(id#545 as string) AS id#554, name#546]                                                                                                                                                 :     +- Filter (id#545 = 1001)
!            :  +- Aggregate [id#545, name#546], [id#545, name#546]                                                                                                                                                  :        +- HiveTableRelation [`default`.`employee`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#545, name#546, salary#547, departmentid#548], Partition Cols: []]
!            :     +- Project [id#545, name#546]                                                                                                                                                                     +- Project [id2#549, name2#550]
!            :        +- Filter (id#545 = cast(1001 as int))                                                                                                                                                            +- HiveTableRelation [`default`.`employee2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id2#549, name2#550, salary#551], Partition Cols: []]
!            :           +- Aggregate [id#545, name#546], [id#545, name#546, max(salary#547) AS max(salary)#553]                                                                                               
!            :              +- HiveTableRelation [`default`.`employee`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#545, name#546, salary#547, departmentid#548], Partition Cols: []]   
!            +- Project [id2#549, name2#550]                                                                                                                                                                   
!               +- HiveTableRelation [`default`.`employee2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id2#549, name2#550, salary#551], Partition Cols: []]                              
          
21/09/05 23:02:38 WARN PlanChangeLogger: 
=== Applying Rule org.apache.spark.sql.catalyst.optimizer.InferFiltersFromConstraints ===
 Aggregate [id#554, name#546, abc], [id#554 AS staffid#542, name#546 AS staffname#543, abc AS level#544]                                                                                           Aggregate [id#554, name#546, abc], [id#554 AS staffid#542, name#546 AS staffname#543, abc AS level#544]
 +- Project [cast(id#545 as string) AS id#554, name#546]                                                                                                                                           +- Project [cast(id#545 as string) AS id#554, name#546]
    +- Join LeftAnti, ((cast(id#545 as string) <=> id2#549) AND (name#546 <=> name2#550))                                                                                                             +- Join LeftAnti, ((cast(id#545 as string) <=> id2#549) AND (name#546 <=> name2#550))
       :- Aggregate [id#545, name#546], [id#545, name#546]                                                                                                                                               :- Aggregate [id#545, name#546], [id#545, name#546]
       :  +- Project [id#545, name#546]                                                                                                                                                                  :  +- Project [id#545, name#546]
!      :     +- Filter (id#545 = 1001)                                                                                                                                                                   :     +- Filter (isnotnull(id#545) AND (id#545 = 1001))
       :        +- HiveTableRelation [`default`.`employee`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#545, name#546, salary#547, departmentid#548], Partition Cols: []]         :        +- HiveTableRelation [`default`.`employee`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#545, name#546, salary#547, departmentid#548], Partition Cols: []]
       +- Project [id2#549, name2#550]                                                                                                                                                                   +- Project [id2#549, name2#550]
          +- HiveTableRelation [`default`.`employee2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id2#549, name2#550, salary#551], Partition Cols: []]                                 +- HiveTableRelation [`default`.`employee2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id2#549, name2#550, salary#551], Partition Cols: []]
           
21/09/05 23:02:38 WARN PlanChangeLogger: 
=== Result of Batch Infer Filters ===
 Aggregate [id#554, name#546, abc], [id#554 AS staffid#542, name#546 AS staffname#543, abc AS level#544]                                                                                           Aggregate [id#554, name#546, abc], [id#554 AS staffid#542, name#546 AS staffname#543, abc AS level#544]
 +- Project [cast(id#545 as string) AS id#554, name#546]                                                                                                                                           +- Project [cast(id#545 as string) AS id#554, name#546]
    +- Join LeftAnti, ((cast(id#545 as string) <=> id2#549) AND (name#546 <=> name2#550))                                                                                                             +- Join LeftAnti, ((cast(id#545 as string) <=> id2#549) AND (name#546 <=> name2#550))
       :- Aggregate [id#545, name#546], [id#545, name#546]                                                                                                                                               :- Aggregate [id#545, name#546], [id#545, name#546]
       :  +- Project [id#545, name#546]                                                                                                                                                                  :  +- Project [id#545, name#546]
!      :     +- Filter (id#545 = 1001)                                                                                                                                                                   :     +- Filter (isnotnull(id#545) AND (id#545 = 1001))
       :        +- HiveTableRelation [`default`.`employee`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#545, name#546, salary#547, departmentid#548], Partition Cols: []]         :        +- HiveTableRelation [`default`.`employee`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#545, name#546, salary#547, departmentid#548], Partition Cols: []]
       +- Project [id2#549, name2#550]                                                                                                                                                                   +- Project [id2#549, name2#550]
          +- HiveTableRelation [`default`.`employee2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id2#549, name2#550, salary#551], Partition Cols: []]                                 +- HiveTableRelation [`default`.`employee2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id2#549, name2#550, salary#551], Partition Cols: []]
          
21/09/05 23:02:38 WARN PlanChangeLogger: Batch Operator Optimization after Inferring Filters has no effect.
21/09/05 23:02:38 WARN PlanChangeLogger: Batch Push extra predicate through join has no effect.
21/09/05 23:02:38 WARN PlanChangeLogger: Batch Early Filter and Projection Push-Down has no effect.
21/09/05 23:02:38 WARN PlanChangeLogger: Batch Update CTE Relation Stats has no effect.
21/09/05 23:02:38 WARN PlanChangeLogger: Batch Join Reorder has no effect.
21/09/05 23:02:38 WARN PlanChangeLogger: Batch Eliminate Sorts has no effect.
21/09/05 23:02:38 WARN PlanChangeLogger: Batch Decimal Optimizations has no effect.
21/09/05 23:02:38 WARN PlanChangeLogger: Batch Distinct Aggregate Rewrite has no effect.
21/09/05 23:02:38 WARN PlanChangeLogger: Batch Object Expressions Optimization has no effect.
21/09/05 23:02:38 WARN PlanChangeLogger: Batch LocalRelation has no effect.
21/09/05 23:02:38 WARN PlanChangeLogger: Batch Check Cartesian Products has no effect.
21/09/05 23:02:38 WARN PlanChangeLogger: Batch RewriteSubquery has no effect.
21/09/05 23:02:38 WARN PlanChangeLogger: Batch NormalizeFloatingNumbers has no effect.
21/09/05 23:02:38 WARN PlanChangeLogger: Batch ReplaceUpdateFieldsExpression has no effect.
21/09/05 23:02:38 WARN PlanChangeLogger: Batch Optimize Metadata Only Query has no effect.
21/09/05 23:02:38 WARN PlanChangeLogger: Batch PartitionPruning has no effect.
21/09/05 23:02:38 WARN PlanChangeLogger: Batch Pushdown Filters from PartitionPruning has no effect.
21/09/05 23:02:38 WARN PlanChangeLogger: Batch Cleanup filters that cannot be pushed down has no effect.
21/09/05 23:02:38 WARN PlanChangeLogger: Batch Extract Python UDFs has no effect.
21/09/05 23:02:38 WARN PlanChangeLogger: Batch User Provided Optimizers has no effect.
21/09/05 23:02:38 WARN PlanChangeLogger: 
=== Metrics of Executed Rules ===
Total number of runs: 267
Total time: 0.117488909 seconds
Total number of effective runs: 18
Total time of effective runs: 0.032170072 seconds
      
21/09/05 23:02:38 WARN PlanChangeLogger: 
=== Applying Rule org.apache.spark.sql.execution.exchange.EnsureRequirements ===
 HashAggregate(keys=[id#554, name#546, abc#558], functions=[], output=[staffid#542, staffname#543, level#544])                                                                                                                                       HashAggregate(keys=[id#554, name#546, abc#558], functions=[], output=[staffid#542, staffname#543, level#544])
!+- HashAggregate(keys=[id#554, name#546, abc AS abc#558], functions=[], output=[id#554, name#546, abc#558])                                                                                                                                         +- Exchange hashpartitioning(id#554, name#546, abc#558, 200), ENSURE_REQUIREMENTS, [id=#2500]
!   +- Project [cast(id#545 as string) AS id#554, name#546]                                                                                                                                                                                             +- HashAggregate(keys=[id#554, name#546, abc AS abc#558], functions=[], output=[id#554, name#546, abc#558])
!      +- SortMergeJoin [coalesce(cast(id#545 as string), ), isnull(cast(id#545 as string)), coalesce(name#546, ), isnull(name#546)], [coalesce(id2#549, ), isnull(id2#549), coalesce(name2#550, ), isnull(name2#550)], LeftAnti                           +- Project [cast(id#545 as string) AS id#554, name#546]
!         :- HashAggregate(keys=[id#545, name#546], functions=[], output=[id#545, name#546])                                                                                                                                                                  +- SortMergeJoin [coalesce(cast(id#545 as string), ), isnull(cast(id#545 as string)), coalesce(name#546, ), isnull(name#546)], [coalesce(id2#549, ), isnull(id2#549), coalesce(name2#550, ), isnull(name2#550)], LeftAnti
!         :  +- HashAggregate(keys=[id#545, name#546], functions=[], output=[id#545, name#546])                                                                                                                                                                  :- Sort [coalesce(cast(id#545 as string), ) ASC NULLS FIRST, isnull(cast(id#545 as string)) ASC NULLS FIRST, coalesce(name#546, ) ASC NULLS FIRST, isnull(name#546) ASC NULLS FIRST], false, 0
!         :     +- Filter (isnotnull(id#545) AND (id#545 = 1001))                                                                                                                                                                                                :  +- Exchange hashpartitioning(coalesce(cast(id#545 as string), ), isnull(cast(id#545 as string)), coalesce(name#546, ), isnull(name#546), 200), ENSURE_REQUIREMENTS, [id=#2492]
!         :        +- Scan hive default.employee [id#545, name#546], HiveTableRelation [`default`.`employee`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#545, name#546, salary#547, departmentid#548], Partition Cols: []]               :     +- HashAggregate(keys=[id#545, name#546], functions=[], output=[id#545, name#546])
!         +- Scan hive default.employee2 [id2#549, name2#550], HiveTableRelation [`default`.`employee2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id2#549, name2#550, salary#551], Partition Cols: []]                                    :        +- Exchange hashpartitioning(id#545, name#546, 200), ENSURE_REQUIREMENTS, [id=#2488]
!                                                                                                                                                                                                                                                                :           +- HashAggregate(keys=[id#545, name#546], functions=[], output=[id#545, name#546])
!                                                                                                                                                                                                                                                                :              +- Filter (isnotnull(id#545) AND (id#545 = 1001))
!                                                                                                                                                                                                                                                                :                 +- Scan hive default.employee [id#545, name#546], HiveTableRelation [`default`.`employee`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#545, name#546, salary#547, departmentid#548], Partition Cols: []]
!                                                                                                                                                                                                                                                                +- Sort [coalesce(id2#549, ) ASC NULLS FIRST, isnull(id2#549) ASC NULLS FIRST, coalesce(name2#550, ) ASC NULLS FIRST, isnull(name2#550) ASC NULLS FIRST], false, 0
!                                                                                                                                                                                                                                                                   +- Exchange hashpartitioning(coalesce(id2#549, ), isnull(id2#549), coalesce(name2#550, ), isnull(name2#550), 200), ENSURE_REQUIREMENTS, [id=#2493]
!                                                                                                                                                                                                                                                                      +- Scan hive default.employee2 [id2#549, name2#550], HiveTableRelation [`default`.`employee2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id2#549, name2#550, salary#551], Partition Cols: []]
           
21/09/05 23:02:38 WARN PlanChangeLogger: 
=== Result of Batch AQE Preparations ===
 HashAggregate(keys=[id#554, name#546, abc#558], functions=[], output=[staffid#542, staffname#543, level#544])                                                                                                                                       HashAggregate(keys=[id#554, name#546, abc#558], functions=[], output=[staffid#542, staffname#543, level#544])
!+- HashAggregate(keys=[id#554, name#546, abc AS abc#558], functions=[], output=[id#554, name#546, abc#558])                                                                                                                                         +- Exchange hashpartitioning(id#554, name#546, abc#558, 200), ENSURE_REQUIREMENTS, [id=#2500]
!   +- Project [cast(id#545 as string) AS id#554, name#546]                                                                                                                                                                                             +- HashAggregate(keys=[id#554, name#546, abc AS abc#558], functions=[], output=[id#554, name#546, abc#558])
!      +- SortMergeJoin [coalesce(cast(id#545 as string), ), isnull(cast(id#545 as string)), coalesce(name#546, ), isnull(name#546)], [coalesce(id2#549, ), isnull(id2#549), coalesce(name2#550, ), isnull(name2#550)], LeftAnti                           +- Project [cast(id#545 as string) AS id#554, name#546]
!         :- HashAggregate(keys=[id#545, name#546], functions=[], output=[id#545, name#546])                                                                                                                                                                  +- SortMergeJoin [coalesce(cast(id#545 as string), ), isnull(cast(id#545 as string)), coalesce(name#546, ), isnull(name#546)], [coalesce(id2#549, ), isnull(id2#549), coalesce(name2#550, ), isnull(name2#550)], LeftAnti
!         :  +- HashAggregate(keys=[id#545, name#546], functions=[], output=[id#545, name#546])                                                                                                                                                                  :- Sort [coalesce(cast(id#545 as string), ) ASC NULLS FIRST, isnull(cast(id#545 as string)) ASC NULLS FIRST, coalesce(name#546, ) ASC NULLS FIRST, isnull(name#546) ASC NULLS FIRST], false, 0
!         :     +- Filter (isnotnull(id#545) AND (id#545 = 1001))                                                                                                                                                                                                :  +- Exchange hashpartitioning(coalesce(cast(id#545 as string), ), isnull(cast(id#545 as string)), coalesce(name#546, ), isnull(name#546), 200), ENSURE_REQUIREMENTS, [id=#2492]
!         :        +- Scan hive default.employee [id#545, name#546], HiveTableRelation [`default`.`employee`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#545, name#546, salary#547, departmentid#548], Partition Cols: []]               :     +- HashAggregate(keys=[id#545, name#546], functions=[], output=[id#545, name#546])
!         +- Scan hive default.employee2 [id2#549, name2#550], HiveTableRelation [`default`.`employee2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id2#549, name2#550, salary#551], Partition Cols: []]                                    :        +- Exchange hashpartitioning(id#545, name#546, 200), ENSURE_REQUIREMENTS, [id=#2488]
!                                                                                                                                                                                                                                                                :           +- HashAggregate(keys=[id#545, name#546], functions=[], output=[id#545, name#546])
!                                                                                                                                                                                                                                                                :              +- Filter (isnotnull(id#545) AND (id#545 = 1001))
!                                                                                                                                                                                                                                                                :                 +- Scan hive default.employee [id#545, name#546], HiveTableRelation [`default`.`employee`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#545, name#546, salary#547, departmentid#548], Partition Cols: []]
!                                                                                                                                                                                                                                                                +- Sort [coalesce(id2#549, ) ASC NULLS FIRST, isnull(id2#549) ASC NULLS FIRST, coalesce(name2#550, ) ASC NULLS FIRST, isnull(name2#550) ASC NULLS FIRST], false, 0
!                                                                                                                                                                                                                                                                   +- Exchange hashpartitioning(coalesce(id2#549, ), isnull(id2#549), coalesce(name2#550, ), isnull(name2#550), 200), ENSURE_REQUIREMENTS, [id=#2493]
!                                                                                                                                                                                                                                                                      +- Scan hive default.employee2 [id2#549, name2#550], HiveTableRelation [`default`.`employee2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id2#549, name2#550, salary#551], Partition Cols: []]
          
21/09/05 23:02:38 WARN PlanChangeLogger: 
=== Applying Rule org.apache.spark.sql.execution.adaptive.InsertAdaptiveSparkPlan ===
!HashAggregate(keys=[id#554, name#546, abc#558], functions=[], output=[staffid#542, staffname#543, level#544])                                                                                                                                       AdaptiveSparkPlan isFinalPlan=false
!+- HashAggregate(keys=[id#554, name#546, abc AS abc#558], functions=[], output=[id#554, name#546, abc#558])                                                                                                                                         +- HashAggregate(keys=[id#554, name#546, abc#558], functions=[], output=[staffid#542, staffname#543, level#544])
!   +- Project [cast(id#545 as string) AS id#554, name#546]                                                                                                                                                                                             +- Exchange hashpartitioning(id#554, name#546, abc#558, 200), ENSURE_REQUIREMENTS, [id=#2500]
!      +- SortMergeJoin [coalesce(cast(id#545 as string), ), isnull(cast(id#545 as string)), coalesce(name#546, ), isnull(name#546)], [coalesce(id2#549, ), isnull(id2#549), coalesce(name2#550, ), isnull(name2#550)], LeftAnti                           +- HashAggregate(keys=[id#554, name#546, abc AS abc#558], functions=[], output=[id#554, name#546, abc#558])
!         :- HashAggregate(keys=[id#545, name#546], functions=[], output=[id#545, name#546])                                                                                                                                                                  +- Project [cast(id#545 as string) AS id#554, name#546]
!         :  +- HashAggregate(keys=[id#545, name#546], functions=[], output=[id#545, name#546])                                                                                                                                                                  +- SortMergeJoin [coalesce(cast(id#545 as string), ), isnull(cast(id#545 as string)), coalesce(name#546, ), isnull(name#546)], [coalesce(id2#549, ), isnull(id2#549), coalesce(name2#550, ), isnull(name2#550)], LeftAnti
!         :     +- Filter (isnotnull(id#545) AND (id#545 = 1001))                                                                                                                                                                                                   :- Sort [coalesce(cast(id#545 as string), ) ASC NULLS FIRST, isnull(cast(id#545 as string)) ASC NULLS FIRST, coalesce(name#546, ) ASC NULLS FIRST, isnull(name#546) ASC NULLS FIRST], false, 0
!         :        +- Scan hive default.employee [id#545, name#546], HiveTableRelation [`default`.`employee`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#545, name#546, salary#547, departmentid#548], Partition Cols: []]                  :  +- Exchange hashpartitioning(coalesce(cast(id#545 as string), ), isnull(cast(id#545 as string)), coalesce(name#546, ), isnull(name#546), 200), ENSURE_REQUIREMENTS, [id=#2492]
!         +- Scan hive default.employee2 [id2#549, name2#550], HiveTableRelation [`default`.`employee2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id2#549, name2#550, salary#551], Partition Cols: []]                                       :     +- HashAggregate(keys=[id#545, name#546], functions=[], output=[id#545, name#546])
!                                                                                                                                                                                                                                                                   :        +- Exchange hashpartitioning(id#545, name#546, 200), ENSURE_REQUIREMENTS, [id=#2488]
!                                                                                                                                                                                                                                                                   :           +- HashAggregate(keys=[id#545, name#546], functions=[], output=[id#545, name#546])
!                                                                                                                                                                                                                                                                   :              +- Filter (isnotnull(id#545) AND (id#545 = 1001))
!                                                                                                                                                                                                                                                                   :                 +- Scan hive default.employee [id#545, name#546], HiveTableRelation [`default`.`employee`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#545, name#546, salary#547, departmentid#548], Partition Cols: []]
!                                                                                                                                                                                                                                                                   +- Sort [coalesce(id2#549, ) ASC NULLS FIRST, isnull(id2#549) ASC NULLS FIRST, coalesce(name2#550, ) ASC NULLS FIRST, isnull(name2#550) ASC NULLS FIRST], false, 0
!                                                                                                                                                                                                                                                                      +- Exchange hashpartitioning(coalesce(id2#549, ), isnull(id2#549), coalesce(name2#550, ), isnull(name2#550), 200), ENSURE_REQUIREMENTS, [id=#2493]
!                                                                                                                                                                                                                                                                         +- Scan hive default.employee2 [id2#549, name2#550], HiveTableRelation [`default`.`employee2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id2#549, name2#550, salary#551], Partition Cols: []]
           
21/09/05 23:02:39 WARN PlanChangeLogger: 
=== Result of Batch Preparations ===
!HashAggregate(keys=[id#554, name#546, abc#558], functions=[], output=[staffid#542, staffname#543, level#544])                                                                                                                                       AdaptiveSparkPlan isFinalPlan=false
!+- HashAggregate(keys=[id#554, name#546, abc AS abc#558], functions=[], output=[id#554, name#546, abc#558])                                                                                                                                         +- HashAggregate(keys=[id#554, name#546, abc#558], functions=[], output=[staffid#542, staffname#543, level#544])
!   +- Project [cast(id#545 as string) AS id#554, name#546]                                                                                                                                                                                             +- Exchange hashpartitioning(id#554, name#546, abc#558, 200), ENSURE_REQUIREMENTS, [id=#2500]
!      +- SortMergeJoin [coalesce(cast(id#545 as string), ), isnull(cast(id#545 as string)), coalesce(name#546, ), isnull(name#546)], [coalesce(id2#549, ), isnull(id2#549), coalesce(name2#550, ), isnull(name2#550)], LeftAnti                           +- HashAggregate(keys=[id#554, name#546, abc AS abc#558], functions=[], output=[id#554, name#546, abc#558])
!         :- HashAggregate(keys=[id#545, name#546], functions=[], output=[id#545, name#546])                                                                                                                                                                  +- Project [cast(id#545 as string) AS id#554, name#546]
!         :  +- HashAggregate(keys=[id#545, name#546], functions=[], output=[id#545, name#546])                                                                                                                                                                  +- SortMergeJoin [coalesce(cast(id#545 as string), ), isnull(cast(id#545 as string)), coalesce(name#546, ), isnull(name#546)], [coalesce(id2#549, ), isnull(id2#549), coalesce(name2#550, ), isnull(name2#550)], LeftAnti
!         :     +- Filter (isnotnull(id#545) AND (id#545 = 1001))                                                                                                                                                                                                   :- Sort [coalesce(cast(id#545 as string), ) ASC NULLS FIRST, isnull(cast(id#545 as string)) ASC NULLS FIRST, coalesce(name#546, ) ASC NULLS FIRST, isnull(name#546) ASC NULLS FIRST], false, 0
!         :        +- Scan hive default.employee [id#545, name#546], HiveTableRelation [`default`.`employee`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#545, name#546, salary#547, departmentid#548], Partition Cols: []]                  :  +- Exchange hashpartitioning(coalesce(cast(id#545 as string), ), isnull(cast(id#545 as string)), coalesce(name#546, ), isnull(name#546), 200), ENSURE_REQUIREMENTS, [id=#2492]
!         +- Scan hive default.employee2 [id2#549, name2#550], HiveTableRelation [`default`.`employee2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id2#549, name2#550, salary#551], Partition Cols: []]                                       :     +- HashAggregate(keys=[id#545, name#546], functions=[], output=[id#545, name#546])
!                                                                                                                                                                                                                                                                   :        +- Exchange hashpartitioning(id#545, name#546, 200), ENSURE_REQUIREMENTS, [id=#2488]
!                                                                                                                                                                                                                                                                   :           +- HashAggregate(keys=[id#545, name#546], functions=[], output=[id#545, name#546])
!                                                                                                                                                                                                                                                                   :              +- Filter (isnotnull(id#545) AND (id#545 = 1001))
!                                                                                                                                                                                                                                                                   :                 +- Scan hive default.employee [id#545, name#546], HiveTableRelation [`default`.`employee`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#545, name#546, salary#547, departmentid#548], Partition Cols: []]
!                                                                                                                                                                                                                                                                   +- Sort [coalesce(id2#549, ) ASC NULLS FIRST, isnull(id2#549) ASC NULLS FIRST, coalesce(name2#550, ) ASC NULLS FIRST, isnull(name2#550) ASC NULLS FIRST], false, 0
!                                                                                                                                                                                                                                                                      +- Exchange hashpartitioning(coalesce(id2#549, ), isnull(id2#549), coalesce(name2#550, ), isnull(name2#550), 200), ENSURE_REQUIREMENTS, [id=#2493]
!                                                                                                                                                                                                                                                                         +- Scan hive default.employee2 [id2#549, name2#550], HiveTableRelation [`default`.`employee2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id2#549, name2#550, salary#551], Partition Cols: []]
          
21/09/05 23:02:39 WARN PlanChangeLogger: Batch AQE Query Stage Optimization has no effect.
21/09/05 23:02:39 WARN PlanChangeLogger: 
=== Applying Rule org.apache.spark.sql.execution.CollapseCodegenStages ===
!Exchange hashpartitioning(id#545, name#546, 200), ENSURE_REQUIREMENTS, [id=#2488]                                                                                                                                                       Exchange hashpartitioning(id#545, name#546, 200), ENSURE_REQUIREMENTS, [id=#2532]
!+- HashAggregate(keys=[id#545, name#546], functions=[], output=[id#545, name#546])                                                                                                                                                      +- *(1) HashAggregate(keys=[id#545, name#546], functions=[], output=[id#545, name#546])
!   +- Filter (isnotnull(id#545) AND (id#545 = 1001))                                                                                                                                                                                       +- *(1) Filter (isnotnull(id#545) AND (id#545 = 1001))
       +- Scan hive default.employee [id#545, name#546], HiveTableRelation [`default`.`employee`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#545, name#546, salary#547, departmentid#548], Partition Cols: []]         +- Scan hive default.employee [id#545, name#546], HiveTableRelation [`default`.`employee`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#545, name#546, salary#547, departmentid#548], Partition Cols: []]
           
21/09/05 23:02:39 WARN PlanChangeLogger: 
=== Result of Batch AQE Post Stage Creation ===
!Exchange hashpartitioning(id#545, name#546, 200), ENSURE_REQUIREMENTS, [id=#2488]                                                                                                                                                       Exchange hashpartitioning(id#545, name#546, 200), ENSURE_REQUIREMENTS, [id=#2532]
!+- HashAggregate(keys=[id#545, name#546], functions=[], output=[id#545, name#546])                                                                                                                                                      +- *(1) HashAggregate(keys=[id#545, name#546], functions=[], output=[id#545, name#546])
!   +- Filter (isnotnull(id#545) AND (id#545 = 1001))                                                                                                                                                                                       +- *(1) Filter (isnotnull(id#545) AND (id#545 = 1001))
       +- Scan hive default.employee [id#545, name#546], HiveTableRelation [`default`.`employee`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#545, name#546, salary#547, departmentid#548], Partition Cols: []]         +- Scan hive default.employee [id#545, name#546], HiveTableRelation [`default`.`employee`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#545, name#546, salary#547, departmentid#548], Partition Cols: []]
          
21/09/05 23:02:39 WARN PlanChangeLogger: Batch AQE Query Stage Optimization has no effect.
21/09/05 23:02:39 WARN PlanChangeLogger: Batch AQE Post Stage Creation has no effect.
21/09/05 23:02:39 WARN PlanChangeLogger: Batch CleanExpressions has no effect.
21/09/05 23:02:39 WARN PlanChangeLogger: 
=== Metrics of Executed Rules ===
Total number of runs: 1
Total time: 5.034E-6 seconds
Total number of effective runs: 0
Total time of effective runs: 0.0 seconds
      
21/09/05 23:02:39 WARN PlanChangeLogger: Batch CleanExpressions has no effect.
21/09/05 23:02:39 WARN PlanChangeLogger: 
=== Metrics of Executed Rules ===
Total number of runs: 1
Total time: 4.614E-6 seconds
Total number of effective runs: 0
Total time of effective runs: 0.0 seconds
      
21/09/05 23:02:39 WARN PlanChangeLogger: Batch Propagate Empty Relations has no effect.
21/09/05 23:02:39 WARN PlanChangeLogger: Batch Dynamic Join Selection has no effect.
21/09/05 23:02:39 WARN PlanChangeLogger: Batch Eliminate Limits has no effect.
21/09/05 23:02:39 WARN PlanChangeLogger: 
=== Metrics of Executed Rules ===
Total number of runs: 5
Total time: 5.57394E-4 seconds
Total number of effective runs: 0
Total time of effective runs: 0.0 seconds
      
21/09/05 23:02:39 WARN PlanChangeLogger: 
=== Applying Rule org.apache.spark.sql.execution.exchange.EnsureRequirements ===
 HashAggregate(keys=[id#554, name#546, abc#559], functions=[], output=[staffid#542, staffname#543, level#544])                                                                                                                                             HashAggregate(keys=[id#554, name#546, abc#559], functions=[], output=[staffid#542, staffname#543, level#544])
!+- HashAggregate(keys=[id#554, name#546, abc AS abc#559], functions=[], output=[id#554, name#546, abc#559])                                                                                                                                               +- Exchange hashpartitioning(id#554, name#546, abc#559, 200), ENSURE_REQUIREMENTS, [id=#2574]
!   +- Project [cast(id#545 as string) AS id#554, name#546]                                                                                                                                                                                                   +- HashAggregate(keys=[id#554, name#546, abc AS abc#559], functions=[], output=[id#554, name#546, abc#559])
!      +- SortMergeJoin [coalesce(cast(id#545 as string), ), isnull(cast(id#545 as string)), coalesce(name#546, ), isnull(name#546)], [coalesce(id2#549, ), isnull(id2#549), coalesce(name2#550, ), isnull(name2#550)], LeftAnti                                 +- Project [cast(id#545 as string) AS id#554, name#546]
!         :- HashAggregate(keys=[id#545, name#546], functions=[], output=[id#545, name#546])                                                                                                                                                                        +- SortMergeJoin [coalesce(cast(id#545 as string), ), isnull(cast(id#545 as string)), coalesce(name#546, ), isnull(name#546)], [coalesce(id2#549, ), isnull(id2#549), coalesce(name2#550, ), isnull(name2#550)], LeftAnti
!         :  +- ShuffleQueryStage 0                                                                                                                                                                                                                                    :- Sort [coalesce(cast(id#545 as string), ) ASC NULLS FIRST, isnull(cast(id#545 as string)) ASC NULLS FIRST, coalesce(name#546, ) ASC NULLS FIRST, isnull(name#546) ASC NULLS FIRST], false, 0
!         :     +- Exchange hashpartitioning(id#545, name#546, 200), ENSURE_REQUIREMENTS, [id=#2532]                                                                                                                                                                   :  +- Exchange hashpartitioning(coalesce(cast(id#545 as string), ), isnull(cast(id#545 as string)), coalesce(name#546, ), isnull(name#546), 200), ENSURE_REQUIREMENTS, [id=#2567]
!         :        +- *(1) HashAggregate(keys=[id#545, name#546], functions=[], output=[id#545, name#546])                                                                                                                                                             :     +- HashAggregate(keys=[id#545, name#546], functions=[], output=[id#545, name#546])
!         :           +- *(1) Filter (isnotnull(id#545) AND (id#545 = 1001))                                                                                                                                                                                           :        +- ShuffleQueryStage 0
!         :              +- Scan hive default.employee [id#545, name#546], HiveTableRelation [`default`.`employee`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#545, name#546, salary#547, departmentid#548], Partition Cols: []]               :           +- Exchange hashpartitioning(id#545, name#546, 200), ENSURE_REQUIREMENTS, [id=#2532]
!         +- ShuffleQueryStage 1                                                                                                                                                                                                                                       :              +- *(1) HashAggregate(keys=[id#545, name#546], functions=[], output=[id#545, name#546])
!            +- Exchange hashpartitioning(coalesce(id2#549, ), isnull(id2#549), coalesce(name2#550, ), isnull(name2#550), 200), ENSURE_REQUIREMENTS, [id=#2493]                                                                                                        :                 +- *(1) Filter (isnotnull(id#545) AND (id#545 = 1001))
!               +- Scan hive default.employee2 [id2#549, name2#550], HiveTableRelation [`default`.`employee2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id2#549, name2#550, salary#551], Partition Cols: []]                                    :                    +- Scan hive default.employee [id#545, name#546], HiveTableRelation [`default`.`employee`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#545, name#546, salary#547, departmentid#548], Partition Cols: []]
!                                                                                                                                                                                                                                                                      +- Sort [coalesce(id2#549, ) ASC NULLS FIRST, isnull(id2#549) ASC NULLS FIRST, coalesce(name2#550, ) ASC NULLS FIRST, isnull(name2#550) ASC NULLS FIRST], false, 0
!                                                                                                                                                                                                                                                                         +- ShuffleQueryStage 1
!                                                                                                                                                                                                                                                                            +- Exchange hashpartitioning(coalesce(id2#549, ), isnull(id2#549), coalesce(name2#550, ), isnull(name2#550), 200), ENSURE_REQUIREMENTS, [id=#2493]
!                                                                                                                                                                                                                                                                               +- Scan hive default.employee2 [id2#549, name2#550], HiveTableRelation [`default`.`employee2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id2#549, name2#550, salary#551], Partition Cols: []]
           
21/09/05 23:02:39 WARN PlanChangeLogger: 
=== Result of Batch AQE Replanning ===
 HashAggregate(keys=[id#554, name#546, abc#559], functions=[], output=[staffid#542, staffname#543, level#544])                                                                                                                                             HashAggregate(keys=[id#554, name#546, abc#559], functions=[], output=[staffid#542, staffname#543, level#544])
!+- HashAggregate(keys=[id#554, name#546, abc AS abc#559], functions=[], output=[id#554, name#546, abc#559])                                                                                                                                               +- Exchange hashpartitioning(id#554, name#546, abc#559, 200), ENSURE_REQUIREMENTS, [id=#2574]
!   +- Project [cast(id#545 as string) AS id#554, name#546]                                                                                                                                                                                                   +- HashAggregate(keys=[id#554, name#546, abc AS abc#559], functions=[], output=[id#554, name#546, abc#559])
!      +- SortMergeJoin [coalesce(cast(id#545 as string), ), isnull(cast(id#545 as string)), coalesce(name#546, ), isnull(name#546)], [coalesce(id2#549, ), isnull(id2#549), coalesce(name2#550, ), isnull(name2#550)], LeftAnti                                 +- Project [cast(id#545 as string) AS id#554, name#546]
!         :- HashAggregate(keys=[id#545, name#546], functions=[], output=[id#545, name#546])                                                                                                                                                                        +- SortMergeJoin [coalesce(cast(id#545 as string), ), isnull(cast(id#545 as string)), coalesce(name#546, ), isnull(name#546)], [coalesce(id2#549, ), isnull(id2#549), coalesce(name2#550, ), isnull(name2#550)], LeftAnti
!         :  +- ShuffleQueryStage 0                                                                                                                                                                                                                                    :- Sort [coalesce(cast(id#545 as string), ) ASC NULLS FIRST, isnull(cast(id#545 as string)) ASC NULLS FIRST, coalesce(name#546, ) ASC NULLS FIRST, isnull(name#546) ASC NULLS FIRST], false, 0
!         :     +- Exchange hashpartitioning(id#545, name#546, 200), ENSURE_REQUIREMENTS, [id=#2532]                                                                                                                                                                   :  +- Exchange hashpartitioning(coalesce(cast(id#545 as string), ), isnull(cast(id#545 as string)), coalesce(name#546, ), isnull(name#546), 200), ENSURE_REQUIREMENTS, [id=#2567]
!         :        +- *(1) HashAggregate(keys=[id#545, name#546], functions=[], output=[id#545, name#546])                                                                                                                                                             :     +- HashAggregate(keys=[id#545, name#546], functions=[], output=[id#545, name#546])
!         :           +- *(1) Filter (isnotnull(id#545) AND (id#545 = 1001))                                                                                                                                                                                           :        +- ShuffleQueryStage 0
!         :              +- Scan hive default.employee [id#545, name#546], HiveTableRelation [`default`.`employee`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#545, name#546, salary#547, departmentid#548], Partition Cols: []]               :           +- Exchange hashpartitioning(id#545, name#546, 200), ENSURE_REQUIREMENTS, [id=#2532]
!         +- ShuffleQueryStage 1                                                                                                                                                                                                                                       :              +- *(1) HashAggregate(keys=[id#545, name#546], functions=[], output=[id#545, name#546])
!            +- Exchange hashpartitioning(coalesce(id2#549, ), isnull(id2#549), coalesce(name2#550, ), isnull(name2#550), 200), ENSURE_REQUIREMENTS, [id=#2493]                                                                                                        :                 +- *(1) Filter (isnotnull(id#545) AND (id#545 = 1001))
!               +- Scan hive default.employee2 [id2#549, name2#550], HiveTableRelation [`default`.`employee2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id2#549, name2#550, salary#551], Partition Cols: []]                                    :                    +- Scan hive default.employee [id#545, name#546], HiveTableRelation [`default`.`employee`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#545, name#546, salary#547, departmentid#548], Partition Cols: []]
!                                                                                                                                                                                                                                                                      +- Sort [coalesce(id2#549, ) ASC NULLS FIRST, isnull(id2#549) ASC NULLS FIRST, coalesce(name2#550, ) ASC NULLS FIRST, isnull(name2#550) ASC NULLS FIRST], false, 0
!                                                                                                                                                                                                                                                                         +- ShuffleQueryStage 1
!                                                                                                                                                                                                                                                                            +- Exchange hashpartitioning(coalesce(id2#549, ), isnull(id2#549), coalesce(name2#550, ), isnull(name2#550), 200), ENSURE_REQUIREMENTS, [id=#2493]
!                                                                                                                                                                                                                                                                               +- Scan hive default.employee2 [id2#549, name2#550], HiveTableRelation [`default`.`employee2`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id2#549, name2#550, salary#551], Partition Cols: []]
          
21/09/05 23:02:39 WARN PlanChangeLogger: 
=== Applying Rule org.apache.spark.sql.execution.adaptive.CoalesceShufflePartitions ===
 HashAggregate(keys=[id#545, name#546], functions=[], output=[id#545, name#546])                                                                                                                                                               HashAggregate(keys=[id#545, name#546], functions=[], output=[id#545, name#546])
!+- ShuffleQueryStage 0                                                                                                                                                                                                                        +- AQEShuffleRead coalesced
!   +- Exchange hashpartitioning(id#545, name#546, 200), ENSURE_REQUIREMENTS, [id=#2532]                                                                                                                                                          +- ShuffleQueryStage 0
!      +- *(1) HashAggregate(keys=[id#545, name#546], functions=[], output=[id#545, name#546])                                                                                                                                                       +- Exchange hashpartitioning(id#545, name#546, 200), ENSURE_REQUIREMENTS, [id=#2532]
!         +- *(1) Filter (isnotnull(id#545) AND (id#545 = 1001))                                                                                                                                                                                        +- *(1) HashAggregate(keys=[id#545, name#546], functions=[], output=[id#545, name#546])
!            +- Scan hive default.employee [id#545, name#546], HiveTableRelation [`default`.`employee`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#545, name#546, salary#547, departmentid#548], Partition Cols: []]               +- *(1) Filter (isnotnull(id#545) AND (id#545 = 1001))
!                                                                                                                                                                                                                                                             +- Scan hive default.employee [id#545, name#546], HiveTableRelation [`default`.`employee`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#545, name#546, salary#547, departmentid#548], Partition Cols: []]
           
21/09/05 23:02:39 WARN PlanChangeLogger: 
=== Result of Batch AQE Query Stage Optimization ===
 HashAggregate(keys=[id#545, name#546], functions=[], output=[id#545, name#546])                                                                                                                                                               HashAggregate(keys=[id#545, name#546], functions=[], output=[id#545, name#546])
!+- ShuffleQueryStage 0                                                                                                                                                                                                                        +- AQEShuffleRead coalesced
!   +- Exchange hashpartitioning(id#545, name#546, 200), ENSURE_REQUIREMENTS, [id=#2532]                                                                                                                                                          +- ShuffleQueryStage 0
!      +- *(1) HashAggregate(keys=[id#545, name#546], functions=[], output=[id#545, name#546])                                                                                                                                                       +- Exchange hashpartitioning(id#545, name#546, 200), ENSURE_REQUIREMENTS, [id=#2532]
!         +- *(1) Filter (isnotnull(id#545) AND (id#545 = 1001))                                                                                                                                                                                        +- *(1) HashAggregate(keys=[id#545, name#546], functions=[], output=[id#545, name#546])
!            +- Scan hive default.employee [id#545, name#546], HiveTableRelation [`default`.`employee`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#545, name#546, salary#547, departmentid#548], Partition Cols: []]               +- *(1) Filter (isnotnull(id#545) AND (id#545 = 1001))
!                                                                                                                                                                                                                                                             +- Scan hive default.employee [id#545, name#546], HiveTableRelation [`default`.`employee`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#545, name#546, salary#547, departmentid#548], Partition Cols: []]
          
21/09/05 23:02:39 WARN PlanChangeLogger: 
=== Applying Rule org.apache.spark.sql.execution.CollapseCodegenStages ===
!Exchange hashpartitioning(coalesce(cast(id#545 as string), ), isnull(cast(id#545 as string)), coalesce(name#546, ), isnull(name#546), 200), ENSURE_REQUIREMENTS, [id=#2594]                                                                         Exchange hashpartitioning(coalesce(cast(id#545 as string), ), isnull(cast(id#545 as string)), coalesce(name#546, ), isnull(name#546), 200), ENSURE_REQUIREMENTS, [id=#2598]
!+- HashAggregate(keys=[id#545, name#546], functions=[], output=[id#545, name#546])                                                                                                                                                                  +- *(2) HashAggregate(keys=[id#545, name#546], functions=[], output=[id#545, name#546])
    +- AQEShuffleRead coalesced                                                                                                                                                                                                                         +- AQEShuffleRead coalesced
       +- ShuffleQueryStage 0                                                                                                                                                                                                                              +- ShuffleQueryStage 0
          +- Exchange hashpartitioning(id#545, name#546, 200), ENSURE_REQUIREMENTS, [id=#2532]                                                                                                                                                                +- Exchange hashpartitioning(id#545, name#546, 200), ENSURE_REQUIREMENTS, [id=#2532]
             +- *(1) HashAggregate(keys=[id#545, name#546], functions=[], output=[id#545, name#546])                                                                                                                                                             +- *(1) HashAggregate(keys=[id#545, name#546], functions=[], output=[id#545, name#546])
                +- *(1) Filter (isnotnull(id#545) AND (id#545 = 1001))                                                                                                                                                                                              +- *(1) Filter (isnotnull(id#545) AND (id#545 = 1001))
                   +- Scan hive default.employee [id#545, name#546], HiveTableRelation [`default`.`employee`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#545, name#546, salary#547, departmentid#548], Partition Cols: []]                     +- Scan hive default.employee [id#545, name#546], HiveTableRelation [`default`.`employee`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#545, name#546, salary#547, departmentid#548], Partition Cols: []]
           
21/09/05 23:02:39 WARN PlanChangeLogger: 
=== Result of Batch AQE Post Stage Creation ===
!Exchange hashpartitioning(coalesce(cast(id#545 as string), ), isnull(cast(id#545 as string)), coalesce(name#546, ), isnull(name#546), 200), ENSURE_REQUIREMENTS, [id=#2594]                                                                         Exchange hashpartitioning(coalesce(cast(id#545 as string), ), isnull(cast(id#545 as string)), coalesce(name#546, ), isnull(name#546), 200), ENSURE_REQUIREMENTS, [id=#2598]
!+- HashAggregate(keys=[id#545, name#546], functions=[], output=[id#545, name#546])                                                                                                                                                                  +- *(2) HashAggregate(keys=[id#545, name#546], functions=[], output=[id#545, name#546])
    +- AQEShuffleRead coalesced                                                                                                                                                                                                                         +- AQEShuffleRead coalesced
       +- ShuffleQueryStage 0                                                                                                                                                                                                                              +- ShuffleQueryStage 0
          +- Exchange hashpartitioning(id#545, name#546, 200), ENSURE_REQUIREMENTS, [id=#2532]                                                                                                                                                                +- Exchange hashpartitioning(id#545, name#546, 200), ENSURE_REQUIREMENTS, [id=#2532]
             +- *(1) HashAggregate(keys=[id#545, name#546], functions=[], output=[id#545, name#546])                                                                                                                                                             +- *(1) HashAggregate(keys=[id#545, name#546], functions=[], output=[id#545, name#546])
                +- *(1) Filter (isnotnull(id#545) AND (id#545 = 1001))                                                                                                                                                                                              +- *(1) Filter (isnotnull(id#545) AND (id#545 = 1001))
                   +- Scan hive default.employee [id#545, name#546], HiveTableRelation [`default`.`employee`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#545, name#546, salary#547, departmentid#548], Partition Cols: []]                     +- Scan hive default.employee [id#545, name#546], HiveTableRelation [`default`.`employee`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#545, name#546, salary#547, departmentid#548], Partition Cols: []]
          
21/09/05 23:02:39 WARN PlanChangeLogger: 
=== Applying Rule org.apache.spark.sql.execution.adaptive.AQEPropagateEmptyRelation ===
 Aggregate [id#554, name#546, abc], [id#554 AS staffid#542, name#546 AS staffname#543, abc AS level#544]                                                                    Aggregate [id#554, name#546, abc], [id#554 AS staffid#542, name#546 AS staffname#543, abc AS level#544]
 +- Project [cast(id#545 as string) AS id#554, name#546]                                                                                                                    +- Project [cast(id#545 as string) AS id#554, name#546]
!   +- Join LeftAnti, ((cast(id#545 as string) <=> id2#549) AND (name#546 <=> name2#550))                                                                                      +- LogicalQueryStage LogicalQueryStage Aggregate [id#545, name#546], [id#545, name#546], HashAggregate(keys=[id#545, name#546], functions=[]), ShuffleQueryStage 2
!      :- LogicalQueryStage LogicalQueryStage Aggregate [id#545, name#546], [id#545, name#546], HashAggregate(keys=[id#545, name#546], functions=[]), ShuffleQueryStage 2   
!      +- LogicalQueryStage Project [id2#549, name2#550], ShuffleQueryStage 1                                                                                               
           
21/09/05 23:02:39 WARN PlanChangeLogger: 
=== Result of Batch Propagate Empty Relations ===
 Aggregate [id#554, name#546, abc], [id#554 AS staffid#542, name#546 AS staffname#543, abc AS level#544]                                                                    Aggregate [id#554, name#546, abc], [id#554 AS staffid#542, name#546 AS staffname#543, abc AS level#544]
 +- Project [cast(id#545 as string) AS id#554, name#546]                                                                                                                    +- Project [cast(id#545 as string) AS id#554, name#546]
!   +- Join LeftAnti, ((cast(id#545 as string) <=> id2#549) AND (name#546 <=> name2#550))                                                                                      +- LogicalQueryStage LogicalQueryStage Aggregate [id#545, name#546], [id#545, name#546], HashAggregate(keys=[id#545, name#546], functions=[]), ShuffleQueryStage 2
!      :- LogicalQueryStage LogicalQueryStage Aggregate [id#545, name#546], [id#545, name#546], HashAggregate(keys=[id#545, name#546], functions=[]), ShuffleQueryStage 2   
!      +- LogicalQueryStage Project [id2#549, name2#550], ShuffleQueryStage 1                                                                                               
          
21/09/05 23:02:39 WARN PlanChangeLogger: Batch Dynamic Join Selection has no effect.
21/09/05 23:02:39 WARN PlanChangeLogger: Batch Eliminate Limits has no effect.
21/09/05 23:02:39 WARN PlanChangeLogger: 
=== Metrics of Executed Rules ===
Total number of runs: 8
Total time: 0.009952953 seconds
Total number of effective runs: 1
Total time of effective runs: 0.009579121 seconds
      
21/09/05 23:02:39 WARN PlanChangeLogger: 
=== Applying Rule org.apache.spark.sql.execution.exchange.EnsureRequirements ===
 HashAggregate(keys=[id#554, name#546, abc#560], functions=[], output=[staffid#542, staffname#543, level#544])                                                                                                                                                   HashAggregate(keys=[id#554, name#546, abc#560], functions=[], output=[staffid#542, staffname#543, level#544])
!+- HashAggregate(keys=[id#554, name#546, abc AS abc#560], functions=[], output=[id#554, name#546, abc#560])                                                                                                                                                     +- Exchange hashpartitioning(id#554, name#546, abc#560, 200), ENSURE_REQUIREMENTS, [id=#2622]
!   +- Project [cast(id#545 as string) AS id#554, name#546]                                                                                                                                                                                                         +- HashAggregate(keys=[id#554, name#546, abc AS abc#560], functions=[], output=[id#554, name#546, abc#560])
!      +- ShuffleQueryStage 2                                                                                                                                                                                                                                          +- Project [cast(id#545 as string) AS id#554, name#546]
!         +- Exchange hashpartitioning(coalesce(cast(id#545 as string), ), isnull(cast(id#545 as string)), coalesce(name#546, ), isnull(name#546), 200), ENSURE_REQUIREMENTS, [id=#2598]                                                                                  +- ShuffleQueryStage 2
!            +- *(2) HashAggregate(keys=[id#545, name#546], functions=[], output=[id#545, name#546])                                                                                                                                                                         +- Exchange hashpartitioning(coalesce(cast(id#545 as string), ), isnull(cast(id#545 as string)), coalesce(name#546, ), isnull(name#546), 200), ENSURE_REQUIREMENTS, [id=#2598]
!               +- AQEShuffleRead coalesced                                                                                                                                                                                                                                     +- *(2) HashAggregate(keys=[id#545, name#546], functions=[], output=[id#545, name#546])
!                  +- ShuffleQueryStage 0                                                                                                                                                                                                                                          +- AQEShuffleRead coalesced
!                     +- Exchange hashpartitioning(id#545, name#546, 200), ENSURE_REQUIREMENTS, [id=#2532]                                                                                                                                                                            +- ShuffleQueryStage 0
!                        +- *(1) HashAggregate(keys=[id#545, name#546], functions=[], output=[id#545, name#546])                                                                                                                                                                         +- Exchange hashpartitioning(id#545, name#546, 200), ENSURE_REQUIREMENTS, [id=#2532]
!                           +- *(1) Filter (isnotnull(id#545) AND (id#545 = 1001))                                                                                                                                                                                                          +- *(1) HashAggregate(keys=[id#545, name#546], functions=[], output=[id#545, name#546])
!                              +- Scan hive default.employee [id#545, name#546], HiveTableRelation [`default`.`employee`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#545, name#546, salary#547, departmentid#548], Partition Cols: []]                                 +- *(1) Filter (isnotnull(id#545) AND (id#545 = 1001))
!                                                                                                                                                                                                                                                                                                 +- Scan hive default.employee [id#545, name#546], HiveTableRelation [`default`.`employee`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#545, name#546, salary#547, departmentid#548], Partition Cols: []]
           
21/09/05 23:02:39 WARN PlanChangeLogger: Batch CleanExpressions has no effect.
21/09/05 23:02:39 WARN PlanChangeLogger: 
=== Metrics of Executed Rules ===
Total number of runs: 1
Total time: 3.5816E-5 seconds
Total number of effective runs: 0
Total time of effective runs: 0.0 seconds
      
21/09/05 23:02:39 WARN PlanChangeLogger: 
=== Result of Batch AQE Replanning ===
 HashAggregate(keys=[id#554, name#546, abc#560], functions=[], output=[staffid#542, staffname#543, level#544])                                                                                                                                                   HashAggregate(keys=[id#554, name#546, abc#560], functions=[], output=[staffid#542, staffname#543, level#544])
!+- HashAggregate(keys=[id#554, name#546, abc AS abc#560], functions=[], output=[id#554, name#546, abc#560])                                                                                                                                                     +- Exchange hashpartitioning(id#554, name#546, abc#560, 200), ENSURE_REQUIREMENTS, [id=#2622]
!   +- Project [cast(id#545 as string) AS id#554, name#546]                                                                                                                                                                                                         +- HashAggregate(keys=[id#554, name#546, abc AS abc#560], functions=[], output=[id#554, name#546, abc#560])
!      +- ShuffleQueryStage 2                                                                                                                                                                                                                                          +- Project [cast(id#545 as string) AS id#554, name#546]
!         +- Exchange hashpartitioning(coalesce(cast(id#545 as string), ), isnull(cast(id#545 as string)), coalesce(name#546, ), isnull(name#546), 200), ENSURE_REQUIREMENTS, [id=#2598]                                                                                  +- ShuffleQueryStage 2
!            +- *(2) HashAggregate(keys=[id#545, name#546], functions=[], output=[id#545, name#546])                                                                                                                                                                         +- Exchange hashpartitioning(coalesce(cast(id#545 as string), ), isnull(cast(id#545 as string)), coalesce(name#546, ), isnull(name#546), 200), ENSURE_REQUIREMENTS, [id=#2598]
!               +- AQEShuffleRead coalesced                                                                                                                                                                                                                                     +- *(2) HashAggregate(keys=[id#545, name#546], functions=[], output=[id#545, name#546])
!                  +- ShuffleQueryStage 0                                                                                                                                                                                                                                          +- AQEShuffleRead coalesced
!                     +- Exchange hashpartitioning(id#545, name#546, 200), ENSURE_REQUIREMENTS, [id=#2532]                                                                                                                                                                            +- ShuffleQueryStage 0
!                        +- *(1) HashAggregate(keys=[id#545, name#546], functions=[], output=[id#545, name#546])                                                                                                                                                                         +- Exchange hashpartitioning(id#545, name#546, 200), ENSURE_REQUIREMENTS, [id=#2532]
!                           +- *(1) Filter (isnotnull(id#545) AND (id#545 = 1001))                                                                                                                                                                                                          +- *(1) HashAggregate(keys=[id#545, name#546], functions=[], output=[id#545, name#546])
!                              +- Scan hive default.employee [id#545, name#546], HiveTableRelation [`default`.`employee`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#545, name#546, salary#547, departmentid#548], Partition Cols: []]                                 +- *(1) Filter (isnotnull(id#545) AND (id#545 = 1001))
!                                                                                                                                                                                                                                                                                                 +- Scan hive default.employee [id#545, name#546], HiveTableRelation [`default`.`employee`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#545, name#546, salary#547, departmentid#548], Partition Cols: []]
          
21/09/05 23:02:39 WARN PlanChangeLogger: Batch CleanExpressions has no effect.
21/09/05 23:02:39 WARN PlanChangeLogger: 
=== Metrics of Executed Rules ===
Total number of runs: 1
Total time: 6.588E-6 seconds
Total number of effective runs: 0
Total time of effective runs: 0.0 seconds
      
21/09/05 23:02:39 WARN PlanChangeLogger: Batch CleanExpressions has no effect.
21/09/05 23:02:39 WARN PlanChangeLogger: 
=== Metrics of Executed Rules ===
Total number of runs: 1
Total time: 7.272E-6 seconds
Total number of effective runs: 0
Total time of effective runs: 0.0 seconds
      
21/09/05 23:02:39 WARN PlanChangeLogger: Batch Propagate Empty Relations has no effect.
21/09/05 23:02:39 WARN PlanChangeLogger: Batch Dynamic Join Selection has no effect.
21/09/05 23:02:39 WARN PlanChangeLogger: Batch Eliminate Limits has no effect.
21/09/05 23:02:39 WARN PlanChangeLogger: 
=== Metrics of Executed Rules ===
Total number of runs: 5
Total time: 1.06531E-4 seconds
Total number of effective runs: 0
Total time of effective runs: 0.0 seconds
      
21/09/05 23:02:39 WARN PlanChangeLogger: 
=== Applying Rule org.apache.spark.sql.execution.exchange.EnsureRequirements ===
 HashAggregate(keys=[id#554, name#546, abc#561], functions=[], output=[staffid#542, staffname#543, level#544])                                                                                                                                                   HashAggregate(keys=[id#554, name#546, abc#561], functions=[], output=[staffid#542, staffname#543, level#544])
!+- HashAggregate(keys=[id#554, name#546, abc AS abc#561], functions=[], output=[id#554, name#546, abc#561])                                                                                                                                                     +- Exchange hashpartitioning(id#554, name#546, abc#561, 200), ENSURE_REQUIREMENTS, [id=#2639]
!   +- Project [cast(id#545 as string) AS id#554, name#546]                                                                                                                                                                                                         +- HashAggregate(keys=[id#554, name#546, abc AS abc#561], functions=[], output=[id#554, name#546, abc#561])
!      +- ShuffleQueryStage 2                                                                                                                                                                                                                                          +- Project [cast(id#545 as string) AS id#554, name#546]
!         +- Exchange hashpartitioning(coalesce(cast(id#545 as string), ), isnull(cast(id#545 as string)), coalesce(name#546, ), isnull(name#546), 200), ENSURE_REQUIREMENTS, [id=#2598]                                                                                  +- ShuffleQueryStage 2
!            +- *(2) HashAggregate(keys=[id#545, name#546], functions=[], output=[id#545, name#546])                                                                                                                                                                         +- Exchange hashpartitioning(coalesce(cast(id#545 as string), ), isnull(cast(id#545 as string)), coalesce(name#546, ), isnull(name#546), 200), ENSURE_REQUIREMENTS, [id=#2598]
!               +- AQEShuffleRead coalesced                                                                                                                                                                                                                                     +- *(2) HashAggregate(keys=[id#545, name#546], functions=[], output=[id#545, name#546])
!                  +- ShuffleQueryStage 0                                                                                                                                                                                                                                          +- AQEShuffleRead coalesced
!                     +- Exchange hashpartitioning(id#545, name#546, 200), ENSURE_REQUIREMENTS, [id=#2532]                                                                                                                                                                            +- ShuffleQueryStage 0
!                        +- *(1) HashAggregate(keys=[id#545, name#546], functions=[], output=[id#545, name#546])                                                                                                                                                                         +- Exchange hashpartitioning(id#545, name#546, 200), ENSURE_REQUIREMENTS, [id=#2532]
!                           +- *(1) Filter (isnotnull(id#545) AND (id#545 = 1001))                                                                                                                                                                                                          +- *(1) HashAggregate(keys=[id#545, name#546], functions=[], output=[id#545, name#546])
!                              +- Scan hive default.employee [id#545, name#546], HiveTableRelation [`default`.`employee`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#545, name#546, salary#547, departmentid#548], Partition Cols: []]                                 +- *(1) Filter (isnotnull(id#545) AND (id#545 = 1001))
!                                                                                                                                                                                                                                                                                                 +- Scan hive default.employee [id#545, name#546], HiveTableRelation [`default`.`employee`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#545, name#546, salary#547, departmentid#548], Partition Cols: []]
           
21/09/05 23:02:39 WARN PlanChangeLogger: 
=== Result of Batch AQE Replanning ===
 HashAggregate(keys=[id#554, name#546, abc#561], functions=[], output=[staffid#542, staffname#543, level#544])                                                                                                                                                   HashAggregate(keys=[id#554, name#546, abc#561], functions=[], output=[staffid#542, staffname#543, level#544])
!+- HashAggregate(keys=[id#554, name#546, abc AS abc#561], functions=[], output=[id#554, name#546, abc#561])                                                                                                                                                     +- Exchange hashpartitioning(id#554, name#546, abc#561, 200), ENSURE_REQUIREMENTS, [id=#2639]
!   +- Project [cast(id#545 as string) AS id#554, name#546]                                                                                                                                                                                                         +- HashAggregate(keys=[id#554, name#546, abc AS abc#561], functions=[], output=[id#554, name#546, abc#561])
!      +- ShuffleQueryStage 2                                                                                                                                                                                                                                          +- Project [cast(id#545 as string) AS id#554, name#546]
!         +- Exchange hashpartitioning(coalesce(cast(id#545 as string), ), isnull(cast(id#545 as string)), coalesce(name#546, ), isnull(name#546), 200), ENSURE_REQUIREMENTS, [id=#2598]                                                                                  +- ShuffleQueryStage 2
!            +- *(2) HashAggregate(keys=[id#545, name#546], functions=[], output=[id#545, name#546])                                                                                                                                                                         +- Exchange hashpartitioning(coalesce(cast(id#545 as string), ), isnull(cast(id#545 as string)), coalesce(name#546, ), isnull(name#546), 200), ENSURE_REQUIREMENTS, [id=#2598]
!               +- AQEShuffleRead coalesced                                                                                                                                                                                                                                     +- *(2) HashAggregate(keys=[id#545, name#546], functions=[], output=[id#545, name#546])
!                  +- ShuffleQueryStage 0                                                                                                                                                                                                                                          +- AQEShuffleRead coalesced
!                     +- Exchange hashpartitioning(id#545, name#546, 200), ENSURE_REQUIREMENTS, [id=#2532]                                                                                                                                                                            +- ShuffleQueryStage 0
!                        +- *(1) HashAggregate(keys=[id#545, name#546], functions=[], output=[id#545, name#546])                                                                                                                                                                         +- Exchange hashpartitioning(id#545, name#546, 200), ENSURE_REQUIREMENTS, [id=#2532]
!                           +- *(1) Filter (isnotnull(id#545) AND (id#545 = 1001))                                                                                                                                                                                                          +- *(1) HashAggregate(keys=[id#545, name#546], functions=[], output=[id#545, name#546])
!                              +- Scan hive default.employee [id#545, name#546], HiveTableRelation [`default`.`employee`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#545, name#546, salary#547, departmentid#548], Partition Cols: []]                                 +- *(1) Filter (isnotnull(id#545) AND (id#545 = 1001))
!                                                                                                                                                                                                                                                                                                 +- Scan hive default.employee [id#545, name#546], HiveTableRelation [`default`.`employee`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#545, name#546, salary#547, departmentid#548], Partition Cols: []]
          
21/09/05 23:02:39 WARN PlanChangeLogger: 
=== Applying Rule org.apache.spark.sql.execution.adaptive.CoalesceShufflePartitions ===
 HashAggregate(keys=[id#554, name#546, abc AS abc#561], functions=[], output=[id#554, name#546, abc#561])                                                                                                                                                     HashAggregate(keys=[id#554, name#546, abc AS abc#561], functions=[], output=[id#554, name#546, abc#561])
 +- Project [cast(id#545 as string) AS id#554, name#546]                                                                                                                                                                                                      +- Project [cast(id#545 as string) AS id#554, name#546]
!   +- ShuffleQueryStage 2                                                                                                                                                                                                                                       +- AQEShuffleRead coalesced
!      +- Exchange hashpartitioning(coalesce(cast(id#545 as string), ), isnull(cast(id#545 as string)), coalesce(name#546, ), isnull(name#546), 200), ENSURE_REQUIREMENTS, [id=#2598]                                                                               +- ShuffleQueryStage 2
!         +- *(2) HashAggregate(keys=[id#545, name#546], functions=[], output=[id#545, name#546])                                                                                                                                                                      +- Exchange hashpartitioning(coalesce(cast(id#545 as string), ), isnull(cast(id#545 as string)), coalesce(name#546, ), isnull(name#546), 200), ENSURE_REQUIREMENTS, [id=#2598]
!            +- AQEShuffleRead coalesced                                                                                                                                                                                                                                  +- *(2) HashAggregate(keys=[id#545, name#546], functions=[], output=[id#545, name#546])
!               +- ShuffleQueryStage 0                                                                                                                                                                                                                                       +- AQEShuffleRead coalesced
!                  +- Exchange hashpartitioning(id#545, name#546, 200), ENSURE_REQUIREMENTS, [id=#2532]                                                                                                                                                                         +- ShuffleQueryStage 0
!                     +- *(1) HashAggregate(keys=[id#545, name#546], functions=[], output=[id#545, name#546])                                                                                                                                                                      +- Exchange hashpartitioning(id#545, name#546, 200), ENSURE_REQUIREMENTS, [id=#2532]
!                        +- *(1) Filter (isnotnull(id#545) AND (id#545 = 1001))                                                                                                                                                                                                       +- *(1) HashAggregate(keys=[id#545, name#546], functions=[], output=[id#545, name#546])
!                           +- Scan hive default.employee [id#545, name#546], HiveTableRelation [`default`.`employee`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#545, name#546, salary#547, departmentid#548], Partition Cols: []]                              +- *(1) Filter (isnotnull(id#545) AND (id#545 = 1001))
!                                                                                                                                                                                                                                                                                           +- Scan hive default.employee [id#545, name#546], HiveTableRelation [`default`.`employee`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#545, name#546, salary#547, departmentid#548], Partition Cols: []]
           
21/09/05 23:02:39 WARN PlanChangeLogger: 
=== Result of Batch AQE Query Stage Optimization ===
 HashAggregate(keys=[id#554, name#546, abc AS abc#561], functions=[], output=[id#554, name#546, abc#561])                                                                                                                                                     HashAggregate(keys=[id#554, name#546, abc AS abc#561], functions=[], output=[id#554, name#546, abc#561])
 +- Project [cast(id#545 as string) AS id#554, name#546]                                                                                                                                                                                                      +- Project [cast(id#545 as string) AS id#554, name#546]
!   +- ShuffleQueryStage 2                                                                                                                                                                                                                                       +- AQEShuffleRead coalesced
!      +- Exchange hashpartitioning(coalesce(cast(id#545 as string), ), isnull(cast(id#545 as string)), coalesce(name#546, ), isnull(name#546), 200), ENSURE_REQUIREMENTS, [id=#2598]                                                                               +- ShuffleQueryStage 2
!         +- *(2) HashAggregate(keys=[id#545, name#546], functions=[], output=[id#545, name#546])                                                                                                                                                                      +- Exchange hashpartitioning(coalesce(cast(id#545 as string), ), isnull(cast(id#545 as string)), coalesce(name#546, ), isnull(name#546), 200), ENSURE_REQUIREMENTS, [id=#2598]
!            +- AQEShuffleRead coalesced                                                                                                                                                                                                                                  +- *(2) HashAggregate(keys=[id#545, name#546], functions=[], output=[id#545, name#546])
!               +- ShuffleQueryStage 0                                                                                                                                                                                                                                       +- AQEShuffleRead coalesced
!                  +- Exchange hashpartitioning(id#545, name#546, 200), ENSURE_REQUIREMENTS, [id=#2532]                                                                                                                                                                         +- ShuffleQueryStage 0
!                     +- *(1) HashAggregate(keys=[id#545, name#546], functions=[], output=[id#545, name#546])                                                                                                                                                                      +- Exchange hashpartitioning(id#545, name#546, 200), ENSURE_REQUIREMENTS, [id=#2532]
!                        +- *(1) Filter (isnotnull(id#545) AND (id#545 = 1001))                                                                                                                                                                                                       +- *(1) HashAggregate(keys=[id#545, name#546], functions=[], output=[id#545, name#546])
!                           +- Scan hive default.employee [id#545, name#546], HiveTableRelation [`default`.`employee`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#545, name#546, salary#547, departmentid#548], Partition Cols: []]                              +- *(1) Filter (isnotnull(id#545) AND (id#545 = 1001))
!                                                                                                                                                                                                                                                                                           +- Scan hive default.employee [id#545, name#546], HiveTableRelation [`default`.`employee`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#545, name#546, salary#547, departmentid#548], Partition Cols: []]
          
21/09/05 23:02:39 WARN PlanChangeLogger: 
=== Applying Rule org.apache.spark.sql.execution.CollapseCodegenStages ===
!Exchange hashpartitioning(id#554, name#546, abc#561, 200), ENSURE_REQUIREMENTS, [id=#2650]                                                                                                                                                                         Exchange hashpartitioning(id#554, name#546, abc#561, 200), ENSURE_REQUIREMENTS, [id=#2655]
!+- HashAggregate(keys=[id#554, name#546, abc AS abc#561], functions=[], output=[id#554, name#546, abc#561])                                                                                                                                                        +- *(3) HashAggregate(keys=[id#554, name#546, abc AS abc#561], functions=[], output=[id#554, name#546, abc#561])
!   +- Project [cast(id#545 as string) AS id#554, name#546]                                                                                                                                                                                                            +- *(3) Project [cast(id#545 as string) AS id#554, name#546]
       +- AQEShuffleRead coalesced                                                                                                                                                                                                                                        +- AQEShuffleRead coalesced
          +- ShuffleQueryStage 2                                                                                                                                                                                                                                             +- ShuffleQueryStage 2
             +- Exchange hashpartitioning(coalesce(cast(id#545 as string), ), isnull(cast(id#545 as string)), coalesce(name#546, ), isnull(name#546), 200), ENSURE_REQUIREMENTS, [id=#2598]                                                                                     +- Exchange hashpartitioning(coalesce(cast(id#545 as string), ), isnull(cast(id#545 as string)), coalesce(name#546, ), isnull(name#546), 200), ENSURE_REQUIREMENTS, [id=#2598]
                +- *(2) HashAggregate(keys=[id#545, name#546], functions=[], output=[id#545, name#546])                                                                                                                                                                            +- *(2) HashAggregate(keys=[id#545, name#546], functions=[], output=[id#545, name#546])
                   +- AQEShuffleRead coalesced                                                                                                                                                                                                                                        +- AQEShuffleRead coalesced
                      +- ShuffleQueryStage 0                                                                                                                                                                                                                                             +- ShuffleQueryStage 0
                         +- Exchange hashpartitioning(id#545, name#546, 200), ENSURE_REQUIREMENTS, [id=#2532]                                                                                                                                                                               +- Exchange hashpartitioning(id#545, name#546, 200), ENSURE_REQUIREMENTS, [id=#2532]
                            +- *(1) HashAggregate(keys=[id#545, name#546], functions=[], output=[id#545, name#546])                                                                                                                                                                            +- *(1) HashAggregate(keys=[id#545, name#546], functions=[], output=[id#545, name#546])
                               +- *(1) Filter (isnotnull(id#545) AND (id#545 = 1001))                                                                                                                                                                                                             +- *(1) Filter (isnotnull(id#545) AND (id#545 = 1001))
                                  +- Scan hive default.employee [id#545, name#546], HiveTableRelation [`default`.`employee`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#545, name#546, salary#547, departmentid#548], Partition Cols: []]                                    +- Scan hive default.employee [id#545, name#546], HiveTableRelation [`default`.`employee`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#545, name#546, salary#547, departmentid#548], Partition Cols: []]
           
21/09/05 23:02:39 WARN PlanChangeLogger: 
=== Result of Batch AQE Post Stage Creation ===
!Exchange hashpartitioning(id#554, name#546, abc#561, 200), ENSURE_REQUIREMENTS, [id=#2650]                                                                                                                                                                         Exchange hashpartitioning(id#554, name#546, abc#561, 200), ENSURE_REQUIREMENTS, [id=#2655]
!+- HashAggregate(keys=[id#554, name#546, abc AS abc#561], functions=[], output=[id#554, name#546, abc#561])                                                                                                                                                        +- *(3) HashAggregate(keys=[id#554, name#546, abc AS abc#561], functions=[], output=[id#554, name#546, abc#561])
!   +- Project [cast(id#545 as string) AS id#554, name#546]                                                                                                                                                                                                            +- *(3) Project [cast(id#545 as string) AS id#554, name#546]
       +- AQEShuffleRead coalesced                                                                                                                                                                                                                                        +- AQEShuffleRead coalesced
          +- ShuffleQueryStage 2                                                                                                                                                                                                                                             +- ShuffleQueryStage 2
             +- Exchange hashpartitioning(coalesce(cast(id#545 as string), ), isnull(cast(id#545 as string)), coalesce(name#546, ), isnull(name#546), 200), ENSURE_REQUIREMENTS, [id=#2598]                                                                                     +- Exchange hashpartitioning(coalesce(cast(id#545 as string), ), isnull(cast(id#545 as string)), coalesce(name#546, ), isnull(name#546), 200), ENSURE_REQUIREMENTS, [id=#2598]
                +- *(2) HashAggregate(keys=[id#545, name#546], functions=[], output=[id#545, name#546])                                                                                                                                                                            +- *(2) HashAggregate(keys=[id#545, name#546], functions=[], output=[id#545, name#546])
                   +- AQEShuffleRead coalesced                                                                                                                                                                                                                                        +- AQEShuffleRead coalesced
                      +- ShuffleQueryStage 0                                                                                                                                                                                                                                             +- ShuffleQueryStage 0
                         +- Exchange hashpartitioning(id#545, name#546, 200), ENSURE_REQUIREMENTS, [id=#2532]                                                                                                                                                                               +- Exchange hashpartitioning(id#545, name#546, 200), ENSURE_REQUIREMENTS, [id=#2532]
                            +- *(1) HashAggregate(keys=[id#545, name#546], functions=[], output=[id#545, name#546])                                                                                                                                                                            +- *(1) HashAggregate(keys=[id#545, name#546], functions=[], output=[id#545, name#546])
                               +- *(1) Filter (isnotnull(id#545) AND (id#545 = 1001))                                                                                                                                                                                                             +- *(1) Filter (isnotnull(id#545) AND (id#545 = 1001))
                                  +- Scan hive default.employee [id#545, name#546], HiveTableRelation [`default`.`employee`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#545, name#546, salary#547, departmentid#548], Partition Cols: []]                                    +- Scan hive default.employee [id#545, name#546], HiveTableRelation [`default`.`employee`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#545, name#546, salary#547, departmentid#548], Partition Cols: []]
          
21/09/05 23:02:39 WARN PlanChangeLogger: Batch CleanExpressions has no effect.
21/09/05 23:02:39 WARN PlanChangeLogger: 
=== Metrics of Executed Rules ===
Total number of runs: 1
Total time: 2.4972E-5 seconds
Total number of effective runs: 0
Total time of effective runs: 0.0 seconds
      
21/09/05 23:02:39 WARN PlanChangeLogger: Batch CleanExpressions has no effect.
21/09/05 23:02:39 WARN PlanChangeLogger: 
=== Metrics of Executed Rules ===
Total number of runs: 1
Total time: 5.554E-6 seconds
Total number of effective runs: 0
Total time of effective runs: 0.0 seconds
      
21/09/05 23:02:39 WARN PlanChangeLogger: Batch CleanExpressions has no effect.
21/09/05 23:02:39 WARN PlanChangeLogger: 
=== Metrics of Executed Rules ===
Total number of runs: 1
Total time: 3.288E-6 seconds
Total number of effective runs: 0
Total time of effective runs: 0.0 seconds
      
21/09/05 23:02:39 WARN PlanChangeLogger: Batch CleanExpressions has no effect.
21/09/05 23:02:39 WARN PlanChangeLogger: 
=== Metrics of Executed Rules ===
Total number of runs: 1
Total time: 2.155E-6 seconds
Total number of effective runs: 0
Total time of effective runs: 0.0 seconds
      
21/09/05 23:02:39 WARN PlanChangeLogger: Batch Propagate Empty Relations has no effect.
21/09/05 23:02:40 WARN PlanChangeLogger: Batch Dynamic Join Selection has no effect.
21/09/05 23:02:40 WARN PlanChangeLogger: Batch Eliminate Limits has no effect.
21/09/05 23:02:40 WARN PlanChangeLogger: 
=== Metrics of Executed Rules ===
Total number of runs: 5
Total time: 7.0579E-5 seconds
Total number of effective runs: 0
Total time of effective runs: 0.0 seconds
      
21/09/05 23:02:40 WARN PlanChangeLogger: Batch AQE Replanning has no effect.
21/09/05 23:02:40 WARN PlanChangeLogger: 
=== Applying Rule org.apache.spark.sql.execution.adaptive.CoalesceShufflePartitions ===
 HashAggregate(keys=[id#554, name#546, abc#561], functions=[], output=[staffid#542, staffname#543, level#544])                                                                                                                                                            HashAggregate(keys=[id#554, name#546, abc#561], functions=[], output=[staffid#542, staffname#543, level#544])
!+- ShuffleQueryStage 3                                                                                                                                                                                                                                                   +- AQEShuffleRead coalesced
!   +- Exchange hashpartitioning(id#554, name#546, abc#561, 200), ENSURE_REQUIREMENTS, [id=#2655]                                                                                                                                                                            +- ShuffleQueryStage 3
!      +- *(3) HashAggregate(keys=[id#554, name#546, abc AS abc#561], functions=[], output=[id#554, name#546, abc#561])                                                                                                                                                         +- Exchange hashpartitioning(id#554, name#546, abc#561, 200), ENSURE_REQUIREMENTS, [id=#2655]
!         +- *(3) Project [cast(id#545 as string) AS id#554, name#546]                                                                                                                                                                                                             +- *(3) HashAggregate(keys=[id#554, name#546, abc AS abc#561], functions=[], output=[id#554, name#546, abc#561])
!            +- AQEShuffleRead coalesced                                                                                                                                                                                                                                              +- *(3) Project [cast(id#545 as string) AS id#554, name#546]
!               +- ShuffleQueryStage 2                                                                                                                                                                                                                                                   +- AQEShuffleRead coalesced
!                  +- Exchange hashpartitioning(coalesce(cast(id#545 as string), ), isnull(cast(id#545 as string)), coalesce(name#546, ), isnull(name#546), 200), ENSURE_REQUIREMENTS, [id=#2598]                                                                                           +- ShuffleQueryStage 2
!                     +- *(2) HashAggregate(keys=[id#545, name#546], functions=[], output=[id#545, name#546])                                                                                                                                                                                  +- Exchange hashpartitioning(coalesce(cast(id#545 as string), ), isnull(cast(id#545 as string)), coalesce(name#546, ), isnull(name#546), 200), ENSURE_REQUIREMENTS, [id=#2598]
!                        +- AQEShuffleRead coalesced                                                                                                                                                                                                                                              +- *(2) HashAggregate(keys=[id#545, name#546], functions=[], output=[id#545, name#546])
!                           +- ShuffleQueryStage 0                                                                                                                                                                                                                                                   +- AQEShuffleRead coalesced
!                              +- Exchange hashpartitioning(id#545, name#546, 200), ENSURE_REQUIREMENTS, [id=#2532]                                                                                                                                                                                     +- ShuffleQueryStage 0
!                                 +- *(1) HashAggregate(keys=[id#545, name#546], functions=[], output=[id#545, name#546])                                                                                                                                                                                  +- Exchange hashpartitioning(id#545, name#546, 200), ENSURE_REQUIREMENTS, [id=#2532]
!                                    +- *(1) Filter (isnotnull(id#545) AND (id#545 = 1001))                                                                                                                                                                                                                   +- *(1) HashAggregate(keys=[id#545, name#546], functions=[], output=[id#545, name#546])
!                                       +- Scan hive default.employee [id#545, name#546], HiveTableRelation [`default`.`employee`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#545, name#546, salary#547, departmentid#548], Partition Cols: []]                                          +- *(1) Filter (isnotnull(id#545) AND (id#545 = 1001))
!                                                                                                                                                                                                                                                                                                                   +- Scan hive default.employee [id#545, name#546], HiveTableRelation [`default`.`employee`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#545, name#546, salary#547, departmentid#548], Partition Cols: []]
           
21/09/05 23:02:40 WARN PlanChangeLogger: 
=== Result of Batch AQE Query Stage Optimization ===
 HashAggregate(keys=[id#554, name#546, abc#561], functions=[], output=[staffid#542, staffname#543, level#544])                                                                                                                                                            HashAggregate(keys=[id#554, name#546, abc#561], functions=[], output=[staffid#542, staffname#543, level#544])
!+- ShuffleQueryStage 3                                                                                                                                                                                                                                                   +- AQEShuffleRead coalesced
!   +- Exchange hashpartitioning(id#554, name#546, abc#561, 200), ENSURE_REQUIREMENTS, [id=#2655]                                                                                                                                                                            +- ShuffleQueryStage 3
!      +- *(3) HashAggregate(keys=[id#554, name#546, abc AS abc#561], functions=[], output=[id#554, name#546, abc#561])                                                                                                                                                         +- Exchange hashpartitioning(id#554, name#546, abc#561, 200), ENSURE_REQUIREMENTS, [id=#2655]
!         +- *(3) Project [cast(id#545 as string) AS id#554, name#546]                                                                                                                                                                                                             +- *(3) HashAggregate(keys=[id#554, name#546, abc AS abc#561], functions=[], output=[id#554, name#546, abc#561])
!            +- AQEShuffleRead coalesced                                                                                                                                                                                                                                              +- *(3) Project [cast(id#545 as string) AS id#554, name#546]
!               +- ShuffleQueryStage 2                                                                                                                                                                                                                                                   +- AQEShuffleRead coalesced
!                  +- Exchange hashpartitioning(coalesce(cast(id#545 as string), ), isnull(cast(id#545 as string)), coalesce(name#546, ), isnull(name#546), 200), ENSURE_REQUIREMENTS, [id=#2598]                                                                                           +- ShuffleQueryStage 2
!                     +- *(2) HashAggregate(keys=[id#545, name#546], functions=[], output=[id#545, name#546])                                                                                                                                                                                  +- Exchange hashpartitioning(coalesce(cast(id#545 as string), ), isnull(cast(id#545 as string)), coalesce(name#546, ), isnull(name#546), 200), ENSURE_REQUIREMENTS, [id=#2598]
!                        +- AQEShuffleRead coalesced                                                                                                                                                                                                                                              +- *(2) HashAggregate(keys=[id#545, name#546], functions=[], output=[id#545, name#546])
!                           +- ShuffleQueryStage 0                                                                                                                                                                                                                                                   +- AQEShuffleRead coalesced
!                              +- Exchange hashpartitioning(id#545, name#546, 200), ENSURE_REQUIREMENTS, [id=#2532]                                                                                                                                                                                     +- ShuffleQueryStage 0
!                                 +- *(1) HashAggregate(keys=[id#545, name#546], functions=[], output=[id#545, name#546])                                                                                                                                                                                  +- Exchange hashpartitioning(id#545, name#546, 200), ENSURE_REQUIREMENTS, [id=#2532]
!                                    +- *(1) Filter (isnotnull(id#545) AND (id#545 = 1001))                                                                                                                                                                                                                   +- *(1) HashAggregate(keys=[id#545, name#546], functions=[], output=[id#545, name#546])
!                                       +- Scan hive default.employee [id#545, name#546], HiveTableRelation [`default`.`employee`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#545, name#546, salary#547, departmentid#548], Partition Cols: []]                                          +- *(1) Filter (isnotnull(id#545) AND (id#545 = 1001))
!                                                                                                                                                                                                                                                                                                                   +- Scan hive default.employee [id#545, name#546], HiveTableRelation [`default`.`employee`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#545, name#546, salary#547, departmentid#548], Partition Cols: []]
          
21/09/05 23:02:40 WARN PlanChangeLogger: 
=== Applying Rule org.apache.spark.sql.execution.CollapseCodegenStages ===
!HashAggregate(keys=[id#554, name#546, abc#561], functions=[], output=[staffid#542, staffname#543, level#544])                                                                                                                                                               *(4) HashAggregate(keys=[id#554, name#546, abc#561], functions=[], output=[staffid#542, staffname#543, level#544])
 +- AQEShuffleRead coalesced                                                                                                                                                                                                                                                 +- AQEShuffleRead coalesced
    +- ShuffleQueryStage 3                                                                                                                                                                                                                                                      +- ShuffleQueryStage 3
       +- Exchange hashpartitioning(id#554, name#546, abc#561, 200), ENSURE_REQUIREMENTS, [id=#2655]                                                                                                                                                                               +- Exchange hashpartitioning(id#554, name#546, abc#561, 200), ENSURE_REQUIREMENTS, [id=#2655]
          +- *(3) HashAggregate(keys=[id#554, name#546, abc AS abc#561], functions=[], output=[id#554, name#546, abc#561])                                                                                                                                                            +- *(3) HashAggregate(keys=[id#554, name#546, abc AS abc#561], functions=[], output=[id#554, name#546, abc#561])
             +- *(3) Project [cast(id#545 as string) AS id#554, name#546]                                                                                                                                                                                                                +- *(3) Project [cast(id#545 as string) AS id#554, name#546]
                +- AQEShuffleRead coalesced                                                                                                                                                                                                                                                 +- AQEShuffleRead coalesced
                   +- ShuffleQueryStage 2                                                                                                                                                                                                                                                      +- ShuffleQueryStage 2
                      +- Exchange hashpartitioning(coalesce(cast(id#545 as string), ), isnull(cast(id#545 as string)), coalesce(name#546, ), isnull(name#546), 200), ENSURE_REQUIREMENTS, [id=#2598]                                                                                              +- Exchange hashpartitioning(coalesce(cast(id#545 as string), ), isnull(cast(id#545 as string)), coalesce(name#546, ), isnull(name#546), 200), ENSURE_REQUIREMENTS, [id=#2598]
                         +- *(2) HashAggregate(keys=[id#545, name#546], functions=[], output=[id#545, name#546])                                                                                                                                                                                     +- *(2) HashAggregate(keys=[id#545, name#546], functions=[], output=[id#545, name#546])
                            +- AQEShuffleRead coalesced                                                                                                                                                                                                                                                 +- AQEShuffleRead coalesced
                               +- ShuffleQueryStage 0                                                                                                                                                                                                                                                      +- ShuffleQueryStage 0
                                  +- Exchange hashpartitioning(id#545, name#546, 200), ENSURE_REQUIREMENTS, [id=#2532]                                                                                                                                                                                        +- Exchange hashpartitioning(id#545, name#546, 200), ENSURE_REQUIREMENTS, [id=#2532]
                                     +- *(1) HashAggregate(keys=[id#545, name#546], functions=[], output=[id#545, name#546])                                                                                                                                                                                     +- *(1) HashAggregate(keys=[id#545, name#546], functions=[], output=[id#545, name#546])
                                        +- *(1) Filter (isnotnull(id#545) AND (id#545 = 1001))                                                                                                                                                                                                                      +- *(1) Filter (isnotnull(id#545) AND (id#545 = 1001))
                                           +- Scan hive default.employee [id#545, name#546], HiveTableRelation [`default`.`employee`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#545, name#546, salary#547, departmentid#548], Partition Cols: []]                                             +- Scan hive default.employee [id#545, name#546], HiveTableRelation [`default`.`employee`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#545, name#546, salary#547, departmentid#548], Partition Cols: []]
           
21/09/05 23:02:40 WARN PlanChangeLogger: 
=== Result of Batch AQE Post Stage Creation ===
!HashAggregate(keys=[id#554, name#546, abc#561], functions=[], output=[staffid#542, staffname#543, level#544])                                                                                                                                                               *(4) HashAggregate(keys=[id#554, name#546, abc#561], functions=[], output=[staffid#542, staffname#543, level#544])
 +- AQEShuffleRead coalesced                                                                                                                                                                                                                                                 +- AQEShuffleRead coalesced
    +- ShuffleQueryStage 3                                                                                                                                                                                                                                                      +- ShuffleQueryStage 3
       +- Exchange hashpartitioning(id#554, name#546, abc#561, 200), ENSURE_REQUIREMENTS, [id=#2655]                                                                                                                                                                               +- Exchange hashpartitioning(id#554, name#546, abc#561, 200), ENSURE_REQUIREMENTS, [id=#2655]
          +- *(3) HashAggregate(keys=[id#554, name#546, abc AS abc#561], functions=[], output=[id#554, name#546, abc#561])                                                                                                                                                            +- *(3) HashAggregate(keys=[id#554, name#546, abc AS abc#561], functions=[], output=[id#554, name#546, abc#561])
             +- *(3) Project [cast(id#545 as string) AS id#554, name#546]                                                                                                                                                                                                                +- *(3) Project [cast(id#545 as string) AS id#554, name#546]
                +- AQEShuffleRead coalesced                                                                                                                                                                                                                                                 +- AQEShuffleRead coalesced
                   +- ShuffleQueryStage 2                                                                                                                                                                                                                                                      +- ShuffleQueryStage 2
                      +- Exchange hashpartitioning(coalesce(cast(id#545 as string), ), isnull(cast(id#545 as string)), coalesce(name#546, ), isnull(name#546), 200), ENSURE_REQUIREMENTS, [id=#2598]                                                                                              +- Exchange hashpartitioning(coalesce(cast(id#545 as string), ), isnull(cast(id#545 as string)), coalesce(name#546, ), isnull(name#546), 200), ENSURE_REQUIREMENTS, [id=#2598]
                         +- *(2) HashAggregate(keys=[id#545, name#546], functions=[], output=[id#545, name#546])                                                                                                                                                                                     +- *(2) HashAggregate(keys=[id#545, name#546], functions=[], output=[id#545, name#546])
                            +- AQEShuffleRead coalesced                                                                                                                                                                                                                                                 +- AQEShuffleRead coalesced
                               +- ShuffleQueryStage 0                                                                                                                                                                                                                                                      +- ShuffleQueryStage 0
                                  +- Exchange hashpartitioning(id#545, name#546, 200), ENSURE_REQUIREMENTS, [id=#2532]                                                                                                                                                                                        +- Exchange hashpartitioning(id#545, name#546, 200), ENSURE_REQUIREMENTS, [id=#2532]
                                     +- *(1) HashAggregate(keys=[id#545, name#546], functions=[], output=[id#545, name#546])                                                                                                                                                                                     +- *(1) HashAggregate(keys=[id#545, name#546], functions=[], output=[id#545, name#546])
                                        +- *(1) Filter (isnotnull(id#545) AND (id#545 = 1001))                                                                                                                                                                                                                      +- *(1) Filter (isnotnull(id#545) AND (id#545 = 1001))
                                           +- Scan hive default.employee [id#545, name#546], HiveTableRelation [`default`.`employee`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#545, name#546, salary#547, departmentid#548], Partition Cols: []]                                             +- Scan hive default.employee [id#545, name#546], HiveTableRelation [`default`.`employee`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Data Cols: [id#545, name#546, salary#547, departmentid#548], Partition Cols: []]
          
21/09/05 23:02:40 WARN PlanChangeLogger: Batch CleanExpressions has no effect.
21/09/05 23:02:40 WARN PlanChangeLogger: 
=== Metrics of Executed Rules ===
Total number of runs: 1
Total time: 7.049E-6 seconds
Total number of effective runs: 0
Total time of effective runs: 0.0 seconds
      
21/09/05 23:02:40 WARN PlanChangeLogger: Batch CleanExpressions has no effect.
21/09/05 23:02:40 WARN PlanChangeLogger: 
=== Metrics of Executed Rules ===
Total number of runs: 1
Total time: 5.506E-6 seconds
Total number of effective runs: 0
Total time of effective runs: 0.0 seconds
      
21/09/05 23:02:40 WARN PlanChangeLogger: Batch CleanExpressions has no effect.
21/09/05 23:02:40 WARN PlanChangeLogger: 
=== Metrics of Executed Rules ===
Total number of runs: 1
Total time: 2.874E-6 seconds
Total number of effective runs: 0
Total time of effective runs: 0.0 seconds
      
Time taken: 2.233 seconds